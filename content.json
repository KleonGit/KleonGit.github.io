{"meta":{"title":"KLEON","subtitle":null,"description":"Think About The Big Map","author":"Kleon","url":"https://blog.kleon.space"},"pages":[{"title":"","date":"2020-10-15T01:03:16.713Z","updated":"2018-12-13T14:53:35.843Z","comments":true,"path":"googlebe23cb0bc55fc412.html","permalink":"https://blog.kleon.space/googlebe23cb0bc55fc412.html","excerpt":"","text":"google-site-verification: googlebe23cb0bc55fc412.html"},{"title":"","date":"2020-12-03T01:45:32.379Z","updated":"2020-12-03T01:45:32.379Z","comments":true,"path":"about/index.html","permalink":"https://blog.kleon.space/about/index.html","excerpt":"","text":"有意思"},{"title":"分类","date":"2020-10-15T01:03:16.701Z","updated":"2020-03-02T12:55:15.229Z","comments":true,"path":"categories/index.html","permalink":"https://blog.kleon.space/categories/index.html","excerpt":"","text":"分类浏览。"},{"title":"标签","date":"2020-10-15T01:03:16.742Z","updated":"2018-11-20T14:14:04.418Z","comments":true,"path":"tags/index.html","permalink":"https://blog.kleon.space/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"机器学习系统 07 - 后端硬件驱动","slug":"ai/ml-system/hardware-backend/7-driver","date":"2021-01-22T01:49:45.000Z","updated":"2021-01-22T02:58:45.096Z","comments":true,"path":"ai/ml-system/hardware-backend/7-driver/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/7-driver/","excerpt":"驱动是对硬件接口的抽象和封装，设计时需要充分考虑分层和解耦。","text":"驱动是对硬件接口的抽象和封装，设计时需要充分考虑分层和解耦。 Linux系统将设备抽象为文件，比如字符设备、块设备和网络接口，保持和文件一致的读写接口。 拓展卡类型的FPGA加速卡使用GPIO和PCIe相连，内置PCIe Controller和DMA提供统一寻址。 DMA提供多组AXI总线（通常与DDR空间对应，DDR空间不足可高位截断）以及一组AXI-lite总线（通常和用户寄存器空间对应）。 FPGA硬件设备驱动的核心模块是DMA驱动，配合scatter-gather DMA，负责数据在主存和设备存储间的交换。 用户软件通过设备文件的读写接口发起大规模DMA读写，通过ioctl等系统调用定制接口，比如寄存器读写等。 FPGA芯片厂商通常提供DMA驱动，对FPGA开发者屏蔽了PCIe/DMA寄存器的操作细节。 因此在驱动和硬件两侧看来，驱动设备文件的数据读写接口和AXI总线接口完全对应。 硬件加速器会基于FPGA厂商提供的驱动对应硬件功能做二次开发，一是向runtime屏蔽硬件操作的复杂性，复用操作逻辑，二是保护硬件，防止用户软件对硬件执行非法操作。 对于深度学习硬件后端，驱动的功能主要分为：内存管理，引擎管理，指令调度等。 内存管理 驱动最简单的管理策略就是直接放开所有物理内存空间，用户可通过读写接口访问任意内存地址。 但这是个危险的操作，多个用户进程可能直接地址冲突，相互干扰，导致硬件逻辑错误。 否则就要修改代码，对每个进程配置一个基地址。 对于同构逻辑可能还行，如果是通用逻辑则无法保证均匀切割的地址空间能满足所有进程的需要。 因此，驱动应提供内存管理的功能，在用户进程发起写请求前，显式申请内存空间，并在进程退出后清理所有残留内存空间。 更为安全的做法是使用虚拟内存，将硬件的物理内存地址空间（有可能不连续）映射到连续的内存空间，相应的返回给用户的是虚拟内存地址。 用户不能直接操作物理内存地址，同时对用户读写请求做内存地址所有权检查，禁止非法访问。 不过通常安全性和性能是相互限制的，在高频读写场景下，可能会影响系统整体吞吐。对于延迟敏感型应用可能有较大影响，应根据实际场景取舍。 可以加入安全模式或低延迟模式的选项，满足不同场景的需要。 内存管理的另一个问题是，内存碎片。 如果采用直白的顺序分配方式，不仅在碎片数量上升的情况下，内存分配时间逐步上升，而且可能会在多轮地址全部分配后出现大量碎片，难以重新分配。 因此一个可以直接想到的方法是，预先将内存切割为大小均等的块，在内存分配时给出适合大小的块，使用bitmap记录内存使用情况。 指令内存和数据内存的大小通常相差很多，可以根据业务场景预设多种尺寸的内存块。这样会浪费一些内存空间， 极端情况是场景所需内存恰好超出预设尺寸一些，必须要分配更大尺寸的空间，不过这样设计的好处是，内存管理逻辑相对简单。 如果要进一步优化内存使用率，可以使用均等切块，不连续内存访问的方式，由驱动控制数据的拆分，可能会将单次DMA读写拆分为多次。 如此引入的额外性能开销，驱动复杂性和调试难度等，需要结合业务通用程度和团队产能平衡。 引擎管理 密集型算子的计算核心对应硬件的计算引擎，同构结构下多个计算引擎的结构与性能完全一致，可能分别置于多个die上（有时候，为了压低单位算力成本需要高超的FPGA后端技巧）。 异构结构可能分为大小核结构和完全异构结构。 使用大小核是为了充分满足设计上限（受限于给定可用资源或面积）。 设计标准核的性能算力最佳，但占用资源较多。 在设计上限内，不能完整容纳另一个标准核，但可以容纳阉割核（砍计算位宽等）。 但大小核功能基本一致，但是性能有差异。 完全异构核是指设计功能不同的计算核，比如拆分计算密集型和长尾通用型计算核，或者针对特定场景（比如推荐）设计特定计算核（比如Embedding）。 驱动设计时需要考虑上述各种情况，可以根据实际应用场景取舍。 一个硬件版本会对应一组硬件结构，可以通过配置文件的形式对应硬件寄存器版本信息。 驱动在初始化设备时，创建对应的数据结构，不同类型的核对应不同的引擎类型。 对外提供引擎分配，释放，启动，重置接口等。 对于完全同构引擎，驱动在分配时可以使用独占或共享策略。 独占策略声明引擎的排他性，通常是处于应用低延迟需要。 由其是在没有引入指令交叉发射的机制下，共享同一个引擎的应用可能会被block较长时间，造成较高的延迟抖动。 共享策略是为了最大化吞吐，多个计算指令填充队列，屏蔽数据交换的overhead。 软件调度 性能优化 Linux驱动基本原理 驱动功能抽象化 数据读写 内存管理 内存碎片化 软件调度的一种可能性 天然跨进程 VPE概念动态调度 硬件调度 驱动性能优化 ioctl开销 DMA队列开销 锁 中断合并","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 06 - 后端硬件软件栈","slug":"ai/ml-system/hardware-backend/6-software-stack-overview","date":"2021-01-21T00:47:50.000Z","updated":"2021-01-21T14:00:25.195Z","comments":true,"path":"ai/ml-system/hardware-backend/6-software-stack-overview/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/6-software-stack-overview/","excerpt":"没有软件支持的硬件就是一块砖，围绕硬件编写通用且易用的软件栈所需的资源不亚于硬件开发。","text":"没有软件支持的硬件就是一块砖，围绕硬件编写通用且易用的软件栈所需的资源不亚于硬件开发。 在硬件工程师主导的团队中，可能会忽视软件的作用以及开发工作量。 因为从硬件的角度来看，软硬件接口的抽象非常明确： 寄存器表 数据 指令 后端硬件通常暴露一组控制寄存器，上层软件通过驱动读写寄存器，完成初始化，复位，状态监测等操作。 后端硬件和主控CPU通过主存交换数据和指令，硬件指令解析器读取指令并发出相应的信号影响控制流和数据流。 然而软件工程需要在软硬件接口之上做多层封装，提供不同层次的接口抽象，满足不同需求的用户。 驱动（Driver） 软件层最底层的封装，与操作系统耦合，提供最基本的硬件控制接口，基本和软硬件接口一致，比如读写寄存器表，读写存储器数据等。 但通常处于安全性考虑，驱动不会直接提供上面的功能，而是进一步封装，提供执行特定功能的接口。一个接口中，可能执行多个寄存器和存储器读写操作。 除此之外，驱动运行于操作系统层，天然具备跨进程的能力，因此在驱动也会负责跨进程的硬件状态管理，比如内存管理，指令调度等。 运行时（Runtime） 用户使用驱动可以最大程度的控制硬件逻辑，但相应的所需要编写的代码量相当可观，因此，驱动层之上还会封装runtime。 runtime通常提供更高层次的抽象接口，进一步屏蔽硬件初始化，上下文（context）管理，设备管理，以及指令/数据的显示加载读取等操作。 总体上来看，runtime的性能相较驱动没有太多损失，但易用性大为提升，错误反馈和排查功能也更为友好。 因此，如果不是对硬件控制有特别高的需求，会使用runtime接口编程。 计算库（Libraries） 编写硬件指令，由其是高效的硬件指令需要对硬件结构有深刻的理解，但是手工编写硬件指令对更高层的用户（比如算法工程师）来说是不够友好并且困难的。 因此后端硬件官方或第三方通常会提供特定领域计算库，提供常见计算pattern的接口，内置凝结了工程师智慧的手工优化代码，比如CuDNN，MKL-DNN等面向深度学习的计算库。 一般来说，硬件厂商提供的计算库性能高于第三方开发的计算库，毕竟对于硬件结构的理解更深入。但是第三方在应用上的创新速度远快于硬件厂商的响应速度，各家厂商对于应用的参与和主导程度也会影响计算库的迭代速度和易用性，比如NVIDIA和Intel。 计算框架（Framework） 计算库提供了特定计算pattern的优化指令，尤其是对计算密集型算子，提高了硬件利用率。但对于一个特定应用来说，还需要足够灵活和高效的计算框架的支持。 对于深度学习应用，计算框架（比如Tensorflow和PyTorch等训练框架）提供了算子级别的抽象，使用Python定义DSL，让用户灵活构建计算图，并提供了反向梯度计算，方便训练。 服务框架用来将训练后的模型部署为在线服务，供业务应用调用，通常使用训练框架的计算核心，但是更注重面向网络接口的高可用，高并发等特性。 后端硬件通常只能支持主流框架全部算子的子集，需要接入框架提高可用性，否则，整个计算图都能在专用硬件上运行的概率不会太高。接入过程主要考虑调度，算子融合，以及常规的编译优化手段。 业务 业务应用通常使用微服务架构，各个模块间相互独立。业务应用负责处理数据处理，格式转换，模型调用等任务。 业务应用从前端被动获取数据（比如用户的行为数据），或者从数据库/数据仓库中取数据，通过RPC1或HTTP调用机器学习模型在线服务，根据服务返回结果（比如物品推荐的打分）执行数据处理操作（比如筛选排列返回给前端的物品呈现顺序），优化某项业务指标（比如点击率或观看时长）。 机器学习服务有SaaS2化的趋势，对小微用户来说，招聘全职的算法工程师和平台工程师的风险偏高，且限于技术深度相对不可控。SaaS服务提供了更高层的使用接口，甚至是无代码操作方式，标准化托管用户的数据收集、处理、分析流程，根据用户的不同需要（比如推荐、风险控制、用户增长等）提供不同类型的服务。 影响 基于数据分析的业务决策已经被实践证明在优化某些特定业务指标时效果明显，但数据分析逐步普及的趋势下，似乎陷入了电影院困境，大家被迫要用数据分析来决策。 数字化确实能带来效率提升，紧随其后的大数据计算利用机器学习在试图在微观层面上找到一些可以带来收益的价值，也就是所谓的挖掘数据中的金子。 智能化是在数据、算力双双获得爆发性增长后的趋势，但目前的智能化仍然只能看做大数据分析的进一步发展形态，依赖算力和数据规模暴力拟合。 因此而消耗的能源和人类时间，给环境造成的影响是否真正创造了等量的价值呢？ 正如模型训练步长过小容易陷入局部最优解，依赖于一个个AB Test做出的短期决策是否对长期价值有贡献呢？ [1] Remote procedure call [2] Software as a service","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 05 - 后端硬件后端流程优化","slug":"ai/ml-system/hardware-backend/5-fpga-backend","date":"2021-01-18T12:28:01.000Z","updated":"2021-01-21T14:00:20.399Z","comments":true,"path":"ai/ml-system/hardware-backend/5-fpga-backend/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/5-fpga-backend/","excerpt":"这里说的性能优化不是计算性能优化，比如脉动阵列的优化，而是如何使硬件设计在FPGA后端流程中时序尽量不违例。","text":"这里说的性能优化不是计算性能优化，比如脉动阵列的优化，而是如何使硬件设计在FPGA后端流程中时序尽量不违例。 FPGA的好处挺多的，可以芯片的价格也是美丽的。资源越多，价格越贵，而且还有不同资源配比，满足不同需要。 一般来说，FPGA中的关键资源包括： DSP（乘加单元），代表了绝对算力，也是有一段短暂的时间FPGA可以和GPU叫板的主要支持因素。 BRAM（片上存储），代表了Cache大小，大到可以把模型整个都放上来，不过实际上只需要放下提前prefetch的数据即可，通过pipeline屏蔽DDR读取latency。 FF（锁存器），代表了寄存器总量，一般不太会不够用吧。 布线资源，影响通用逻辑复杂程度。 LUT（查找表），影响通用逻辑复杂程度。 那个时候，大家的工艺节点还差不多，Xilinx猛怼DSP，单个封装内3D堆几个die，加上FPGA工程师使用的倍频技巧，实际计算效率还能和GPU比一比。 后来，NVIDIA财大气粗，逐步垄断市场，算力每代大跃进。Xilinx起初还想以ACAP1——同时集成ARM、FPGA、CGRA的巨兽——反超。 但无奈ACAP还是难产，大部分时间停留在励志吃下5G、自动驾驶、人工智能等多个领域的PPT中。即使真的大规模量产，成本也是可以想象的没有竞争力。最终，做FPGA加速器的，还是老老实实回去做芯片了。 FPGA后端指的是Synthesize和Implementation，Implementation包括Remapping，Placing、Routing等。 相比芯片要傻瓜的多。通过XDC约束，更换策略，画pblock等方法降低违例TNS（Total Negative Slack）。 通过阅读STA报告，我们基本能定位有问题（比如fan out过高，WNS明显过小，拥塞等级过高）的地方，反过来修改设计，比如插入寄存器，对跨时钟的逻辑set_false_path，或者手动复制寄存器等等。 DSP一般被用来搭建计算密集型算子，元素积或者自然指数通常用LUT搭乘法器，根据频率调整合适的pipeline级数，前后插入寄存器提高时序。 控制跨die信号总量，跨die频率尽可能的低，前后加寄存器。 多阅读官方文档，熟悉底层器件特性，明白自己写的代码映射到什么样的硬件结构上，参考示例工程。 [1] Versal","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 04 - 后端硬件验证","slug":"ai/ml-system/hardware-backend/4-verification","date":"2021-01-15T08:12:01.000Z","updated":"2021-01-21T14:00:15.750Z","comments":true,"path":"ai/ml-system/hardware-backend/4-verification/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/4-verification/","excerpt":"仿真不是真，只不过是现实世界的一个有限切面。但仿真又是在进入现实世界前的必经试炼。","text":"仿真不是真，只不过是现实世界的一个有限切面。但仿真又是在进入现实世界前的必经试炼。 仿真（Emulation）用来从底层机制层面模拟硬件运行性能，测试功能正确性，评估性能是否达到设计标准。 层级 项目启动阶段重心放在硬件功能开发上，而验证流程可能处于刀耕火种阶段： C++写Test Case，dump输入数据文件和输出结果文件，要求转化成Verilog-friendly 16进制字符串 手写Verilog读取文件，围绕仿真激励逻辑，dump波形文件和输出结果文件 diff比对Test Case和Verilog的输出结果文件，匹配不上需要看波形定位问题 实际生产中需要完善的仿真测试来保证达到设计目标，尤其是芯片，毕竟比较贵。 如果流片回来变砖就很尴尬了，赶在流片前，尽力而为。 和软件测试类似，硬件验证也分为不同层级。根据粒度可以将仿真分为： 单元仿真 测试基础功能模块是否达到设计目标，通常设计一些测试用例（Testbench）来验证，基本作用与软件单元测试（Unit Test）类似。 软件测试的基础单元是函数（Function），硬件测试的基础单元是模块（Module），需要选择合适的测试粒度大小。 TDD1（Test Driven Development）适合拆解开发工作量，也可以用于硬件开发。 由于HDL及其周边工具的迭代速度缓慢，语言编程接口与仿真调试工具的易用性与软件相比，并没有显著性提升。 因此在以开发速度远超ASIC闻名的FPGA开发流程中，单元仿真可以是不存在的。 至少在一个项目进度很紧的早期团队中是不存在的。 系统仿真 这里基本实现了较独立的完整功能。为了提高仿真速度，一般去除比较独立的寄存器逻辑，仿真非常耗时的存储器（如DDR仿真模型）逻辑等。 即便去除了一些耗时的逻辑，硬件仿真的耗时相比软件高出不少，并且还没有测试Cache机制，每运行一次可能需要数个小时，导出的波形文件也非常大。 除了一些用来跑通控制流和数据流的Case之外，一般也会采用遍历（case较少）或者随机测试，最终看代码覆盖率。 如果要做比较完善的测试，靠刀耕火种的流程肯定是不够的，自动化（Automation）是必须的。 后仿真 后仿真，是指可以在每个后端流程（综合，布局布线）后抽取HDL仿真。用于检查流程是否出错，以及时序验证。 前端也可以手动加入一个delay（不会被综合，但可以指导仿真），模拟信号固定延迟，波形上好看一些。 FPGA几乎从不后仿，非常费时，不如可以直接上板，通常一些只有上板才出现的问题也仿不出来。 上板调试 通常使用FPGA开发板，因此称为上板调试。在芯片验证流程中，FPGA被用来做原型验证，一般降频运行，测试功能时序正确性。 容易遇到的问题是，测试模型需要用软件重写，导致引入额外bug。 如果真的遇到了和预期不符的地方，调试起来就比较费力。 需要使用FPGA IDE的信号抓取功能添加信号线作为debug逻辑，再重新跑后端流程。 当FPGA项目比较大时，一次可以抓取的信号数量有限，需要同时编很多版本。 在FPGA资源已经比较吃紧的情况下，拥塞程度较高，添加过多信号线，会让时序优化更有压力。 实际测试下来发现，很多bug都是因为Test Case不够全。 因此要尽可能在之前的仿真中测出bug啊！ 如果遇到了抓信号也不好复现的bug： 有可能是跨时钟问题，这个在仿真中不太好复现。 有极小可能（真的碰到过）是定制板卡的PCB版设计有问题。 几乎没有可能是因为宇宙射线引起的单粒子反转效应。😃 应用 测试框架可以使用System Verilog的UVM2（常用的芯片工业的验证框架）或者基于Python的Cocotb3。 深度学习的FPGA后端的核心功能是offload计算密集型算子，并且与标准框架（Tensorflow，PyTorch）实现比对数据，对验证的覆盖性要求不高。可以使用基于Python的Cocotb，使用Python编写测试逻辑的好处是有大量可用包，写数据处理逻辑比较方便。 基于COCOTB的协同仿真框架 Cocotb只提供了核心组件和System Verilog的VPI4连接，需要编写硬件激励的Driver，把Python数据转换成硬件信号，比如AXI/AXI-lite Driver；编写信号采样Probe，把硬件信号转换成Python数据。 向上封装类DMA的驱动接口，包括DMA数据读写以及寄存器读写。这样在这个层面可以做到和驱动接口统一，便于在上板测试时保持上层激励逻辑不变。 再向上封装硬件相关的运行时（Runtime）模型，直接配置硬件指令，运行时模型负责模拟内存分配，数据和指令读写等功能。使用者只需要指令遍历指令的集合验证即可。 由于Verilog仿真速度很慢，在指令遍历选项过多的情况下会非常耗时，一般选择直接上板遍历，并记录对应失败配置，有针对性地对失败的case仿真调试，查看波形。 收获的一个经验是： 测试越早写，收获越大，可以有效减少系统调试时一些花了很久定位的“显而易见”的bug。但目前还不知道怎么能提高写测试的积极性，降低写测试的overhead。 绝大多数代码只会被用一次。 [1] Test-driven development [2] Universal Verification Methodology [3] cocotb/cocotb [4] Verilog Procedural Interface","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 03 - 后端硬件描述语言","slug":"ai/ml-system/hardware-backend/3-hardware-description-language","date":"2021-01-14T08:52:01.000Z","updated":"2021-01-21T14:00:12.303Z","comments":true,"path":"ai/ml-system/hardware-backend/3-hardware-description-language/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/3-hardware-description-language/","excerpt":"语言只是手段，不是目的。自然语言如此，软件编程语言如此，硬件描述语言也如此。","text":"语言只是手段，不是目的。自然语言如此，软件编程语言如此，硬件描述语言也如此。 软件编程语言定义了一系列面向人类可读性的字符语法。 人类使用软件编程语言编写描述机器如何处理信息的步骤的文本，简称为软件代码。 使用编译软件使用指定指令集翻译成机器具体处理信息的指令，简称为程序。🤓 类似的，在硬件设计中，硬件的结构决定了功能，因此也需要借助一种文本描述硬件结构，得名硬件（结构）描述语言。 软件代码可以直接编译成程序运行，也可以直接和测试框架一起编译运行，测试功能正确性。 而硬件描述语言（HDL，Hardware Description Language）——其中对应硬件结构（可综合）的合法语法——只是对硬件的结构描述，需要使用仿真器（Emulator）构造硬件结构。 通常高效的仿真器会将硬件代码等价映射成软件代码（比如C++）编译后运行，提高仿真效率。 就可综合语法部分看，HDL乏像软件语言中的抽象模型，以用于代码复用。 但是HDL中的寄存器、线以及模型足以描述任何复杂的硬件。 可以利用软件工具管理生成代码，减少一部分coding的工作量。 比如使用脚本（Perl或者Python）管理模块，辅助连线，或者借用软件的OOP（Object Oriented Programming)模型，套壳做DSL（Domain Specific Language）（比如Chisel套在Scala里），借助套壳语言的语法和抽象简化硬件编码工作。 除了可综合语法，HDL中还引入了测试语法用来提供仿真激励。为了改进HDL的测试方面的短板，更多语法和抽象被引入进来，以至于可以被称为一门新的语言（比如System Verilog1)。 有趣的是，System Verilog可以被看作天然的并行语言，引入不少并发机制，比如channel，GoLang的并发机制里也有channel。 硬件开发的主要工作量并不在前端编码（主要包括编写Verilog并做功能仿真），占大头的是仿真测试和优化。 在一些非自主开发IP的流程中，前端的工作基本就是把买来的IP（比如ARM）集成连线，写一些胶水逻辑。 在一些自主IP定制的流程中，比如RISC-V，主要的功能还是在拓展指令集，然后接着大量测试和优化。 我曾经试图发明一门新的语言，使用现代语言的语法来改进Verilog。写好了词法Parser，拿到了AST，做了一些类型推断以及代码优化工作，可以生成Verilog。然而如果要用于生产，开发工作量是不小的。最终只是作为一个toy项目，帮我熟悉了一些编译器的知识。 现在回过头来看，如果考虑写一门HDL的话，需要考虑几点： 是否可以适配已有工业流程？通常是生成Verilog和其他EDA流程对接。 是否可以做到和Verilog可综合语法完全等价？需要形式化验证以及实际项目支持（比如Chisel傍上了RISC-V）。 是否支持其他blackbox（加密IP网表，VHDL，Verilog，SystemVerilog仿真模型）联合仿真？ 是否能在保证仿真结果精度准确的情况下，提高仿真效率？只能粗粒度功能仿真就很鸡肋。 是否可以引入稳定的自动优化？怀疑是否又必要，后期优化的时候基本会扣每个寄存器，后端流程也有大量优化。 是否易学？过多的引入严格的抽象概念会大幅提高语言学习难度（用软件语言的角度类比，学go，可能3天上手写工程，学rust，学一个月可能还经常卡在工程编译上），HDL依赖惯性可能比软件语言要大的多。 软件语言的迭代，引入了新的编程模型，新的内存管理机制，新的并行模型等等，实实在在地带来了极大的收益，大幅提高了并发性能，大幅减少了代码量，大幅降低了运行时排错的成本等等。 最终决定新语言是否会被大规模使用的核心还是：开发学习使用新语言引入的成本是否带来了足够的生产力收益。 HDL的核心功能还是描述硬件结构。 如果高层语言可以直接生成Verilog，要么是面向FPGA的HLS2（High Level Synthesis），面向软件工程师的硬件加速，根本不关心生成的代码，生成的代码也不可读。 要么是面向芯片的设计流程，语言本身最好能和硬件资源一一对应，所见即所得，否则就要生成可读的代码，让工程师检查。不过工程师既然都要人肉检查生成的代码了，再套一层壳就比较鸡肋了。 HDL已经深深嵌入到整个芯片设计流程中，前端和后端都能统一，EDA工具掌管着优化，VCS和Verdi已经是测试的标准工具。相比软件语言，HDL是一个非常细分且小众的DSL。 也许，之后会有新的HDL成为事实的工业界标准，但应该会是一个漫长的过程。 [1] System Verilog [2] High-level synthesis","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 02 - 后端硬件指令集","slug":"ai/ml-system/hardware-backend/2-instruction-set","date":"2021-01-13T07:14:01.000Z","updated":"2021-01-21T14:00:08.795Z","comments":true,"path":"ai/ml-system/hardware-backend/2-instruction-set/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/2-instruction-set/","excerpt":"定制计算硬件的一个重要任务是确定指令集1。对于深度学习模型，则要首先确定模型集，继而确定算子集，最后统计出指令集。","text":"定制计算硬件的一个重要任务是确定指令集1。对于深度学习模型，则要首先确定模型集，继而确定算子集，最后统计出指令集。 划分 总体上看，指令按计算密度分为计算密集型算子指令与长尾算子指令。 计算密集型算子指令通常就是指矩阵乘加计算，其他密集型算子（比如深度学习定义的卷积运算）可以通过变换转化为矩阵乘加。 因此矩阵乘加是定制硬件的核心，也是模型优化的核心。 换句话说，定制硬件（包括GPGPU）的主要设计工作在于如何在面积功耗约束下放下更多的矩阵乘加单元，同时提供匹配算力的存储访问带宽。 模型优化的主要内容就是在于如何合理排布硬件的矩阵计算指令和存储访问指令，使矩阵计算模型的实际利用率（有效计算时间占总运行时间）尽可能接近硬件的理论算力上限。 长尾算子指令则是针对出现在模型中但计算量不太大的算子设计的指令，但总体而言模型的复杂程度和改进速度都比硬件开发速度快，不可能浪费芯片面积支持所有的长尾算子，因此设计目标按优先级排序是： 如果长尾算子不在模型首尾，而出现在两个密集算子之间，并且CPU计算开销（通常指耗时）相比交换给CPU计算（内存交换以及中断/轮询）的开销小，就应该尽量使用硬件计算。 归纳必须支持的算子，抽象出更通用的计算逻辑，尽量最小化占用的芯片面积。适配计算密集型指令算力和长尾指令算力，在多种目标场景中统计，长尾指令尽量不成为性能瓶颈。 评估大型模型（不能完全放在芯片内存中，需要和CPU主存交换）在目标承载系统中的算力适配程度，如CPU算力是否能支持内存拷贝开销，中断开销，硬件不支持的长尾算子的计算开销，系统其他服务开销。 粒度 指令是硬件向软件开放的“操作接口”，指令的粒度则决定了软件对硬件控制深度。 定制硬件的早期，尤其是针对某个特定模型的特定设计，软件和硬件都是及其专用的。硬件基本包揽了模型端到端的所有计算，甚至连模型结构也以一种“硬编码”的方式写在了驱动中。 硬件的整体架构只有两部分：一部分是寄存器，给软件配置来启动计算步骤，另一部分是一条巨大的流水线，归纳了整个模型最基本的计算结构，比如对于ResNet50，就是卷积-元素积-仿射变换-激活ReLU-池化。 软件运行时（Runtime）部分几乎就是很薄的一层，甚至可以大部分写在驱动中。驱动主要负责启动DMA读写数据以及配置查询寄存器。运行时只需要从某个地址读入输入数据，配置一个寄存器，接收到中断信号后读回数据写到另一个地址。 如果把配置寄存器也看作指令的话，那这就是最粗粒度的指令，软件没有控制的余地，指令可以写作RESNET50 🙂 。 逐渐地，需要支持更多相似模型，不同层数，不同长尾算子，则需要按层来配置卷积大小，长尾算子是否计算，有无pypass，内存数据如何存放等。硬件的流水线也配置了更多寄存器开关支持不同的计算路径。 这就是单层粒度的指令，软件可以通过一大组寄存器灵活配置每层的属性，也有了多模型interleaving（同时交叉）计算的空间。此时的指令可以写作LOAD-CONV-ELTMUL-AFF-RELU-POOLING-STORE 🙂 。类似粒度的硬件可以参考NVDLA2。 软件的功能逐渐增多，在运行时（Runtime）上增加编译器部分，将相近的算子变换为硬件支持的指令。硬件后端支持的模型更多了，但实际计算效率可能有所下降。这也是一个常识————通用性会牺牲专用性的性能，比如Windows和MacOS，CPU和GPU。但群体智慧的迭代可以逐步弥补通用性带来的损失，比如RISC与CISC，Android与iOS。 随着硬件的通用化，几乎不可避免地会向主流已验证设计靠拢，卷积计算将不再特例化，不断增加的变种支持会使硬件复杂度越来越来高。最终不得不回归本源————矩阵乘加，原本的超长流水线设计也会自然分化成两部分————密集计算单元与长尾计算单元指令。 密集计算单元由于去除了特例化设计，密度可以进一步提高；长尾计算单元将进一步整合，提高通用性。此时的指令进一步细化为LOAD，MATMUL，MULT，ADD，EXP，STORE等。指令操作的精细程度也进一步提高，类似SIMD操作。 多个计算单元和局部缓存构成一个计算引擎或者计算核，多个核可以由软件或者硬件直接调度（指令发射顺序），编译器的复杂度进一步上升，可以在更细粒度调整指令顺序，考虑缓存以及内存带宽匹配（Memory-Bound）。到了这个粒度的指令离GPGPU也不远了。 指令集基本划定了硬件可能的应用边界，不过实际硬件设计实现过程中还有其他要考虑的因素，取决于芯片后端与FPGA后端需求，略。 [1] 指令集架构 [2] NVIDIA Deep Learning Accelerator","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 01 - 后端硬件初探","slug":"ai/ml-system/hardware-backend/1-overview","date":"2021-01-12T04:58:36.000Z","updated":"2021-01-21T14:00:01.597Z","comments":true,"path":"ai/ml-system/hardware-backend/1-overview/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/1-overview/","excerpt":"机器学习系统通常使用定制硬件后端执行计算密集型算子。训练模型中包含大量训练专用的反向梯度算子，相比之下推理模型结构更简单，算子集更小。因此定制硬件通常从推理模型入手。","text":"机器学习系统通常使用定制硬件后端执行计算密集型算子。训练模型中包含大量训练专用的反向梯度算子，相比之下推理模型结构更简单，算子集更小。因此定制硬件通常从推理模型入手。 分类 如果采用异构计算系统的定义，硬件后端可以大致分为： CPU 对应最通用的计算设备，可以执行所有计算，灵活性最高，计算性能通常不够高。 不过由于CPU的广泛通用性以及较低的成本，使用CPU做推理计算也很常见。 因此针对不同指令集的CPU也发展出了不同的优化方案，比如Intel加入了AVX512指令支持超宽SIMD指令，但直接加入编译选项对性能提升不明显，需要使用集合了Intel汇编工程师的智慧的MKL-DNN库，针对不同算子精心设计精细到cache-line的操作；Intel随后几代CPU又加入了VNNI指令（INT8支持，实际使用时也有限制）和BF16指令（详细见推理优化）。 GPGPU1 对应计算密集型通用计算设备，可以执行有限的通用计算，取决于设备指令集。市场现状是，NVIDIA几乎垄断服务器级GPGPU供给，成本较高。CUDA软件栈闭源，优化通常只能基于NVIDIA提供的接口。不过也有一些奇技淫巧，通过接口劫持绕过CUDA软件的部分限制，需要一定的逆向工程的能力。 NVIDIA在模型量化（使用低位宽存储模型参数）上也很激进，INT8，INT4，FP16领先加入硬件。随之也出现过一阵超低精度paper热，使用2bit/1bit存储。如今，量化方案基本成熟，使用FP32训练，使用FP32直接推理，使用FP16/BF16推理精度略有，使用INT8则可能需要重新训练以恢复精度（详细见模型压缩）。 NPU 对应最专用的计算设备，通常支持的指令集最小，比如仅支持矩阵乘法，卷积，以及较常用的高频算子，执行效率最高。 几年前（现在是2020年）FPGA2凭借其相比ASIC3（专用定制芯片）的灵活可编程能力，一度出圈，成为深度学习硬件加速器的宠儿。不过，如今泡沫退去，一大批ASIC创业公司纷纷倒闭，FPGA自然也退回其最基础的应用场景（原型验证，低频可编程硬件）。 与此同时，NPU作为芯片的必备模块，已经与ARM整合进入手机CPU。一些大公司财大气粗，内部也在使用自研的特定场景芯片。 专用硬件突出“专用”二字，这意味着可以将应用场景无限缩窄。可以针对一类模型，甚至一种模型，甚至几种算子，针对性地使用大量专用优化。这样固然可能取得不错的性能，但是硬件开发本身相比软件来说慢得多。芯片的开发周期通常以年计，FPGA开发可能以月计，软件开发通常以周计。相较之下，必须要权衡开发时间、性能、成本以及通用性。 FPGA在芯片原型阶段的优势明显，如果大规模量产则成本与功耗都远高于ASIC。 另一方面，随着GPGPU的不断发展，先进工艺节点夹持，性能大幅提高，FPGA早期的低成本性能比的优势不复存在。在泡沫时期可能存在诸多在多项指标中平衡的特定场景硬件，但最终还是互相合并，收敛到几种硬件设计方案。 不过FPGA作为不错的早期原型平台还是有价值的，前端硬件的设计思路也和芯片设计相通，可以借鉴。 [1] 图形处理器通用计算 [2] Field-programmable gate array [3] Application-specific integrated circuit","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"}]},{"title":"机器学习系统 00 - 总览","slug":"ai/ml-system/0-overview","date":"2021-01-11T01:08:04.000Z","updated":"2021-01-21T01:49:56.713Z","comments":true,"path":"ai/ml-system/0-overview/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/0-overview/","excerpt":"机器学习1模型对应一类用于从数据中提取模式与特征的人工智能子领域算法。机器学习系统用于更便捷地、更高效地、更自动化地构建机器学习算法。","text":"机器学习1模型对应一类用于从数据中提取模式与特征的人工智能子领域算法。机器学习系统用于更便捷地、更高效地、更自动化地构建机器学习算法。 机器学习是数据分析的延展，得益于算法改进（Algorithm)，数据累积增加（Big Data），算力提升（Computing Power），深度学习模型被大规模应用。 随之而来的，是对快捷便利地构建部署深度学习的需求，机器学习系统也随之迭代优化。 架构 机器学习系统整体架构 机器学习系统根据构建阶段可以分为： 数据管理 机器学习的核心是数据，这些数据通常来自业务系统的数据仓库，通过ETL2系统清洗提取后组织成统一的结构化数据。 数据管理子系统用来管理用户导入的结构化数据，称为数据集（Dataset）。通常数据管理子系统提供数据标注功能，由数据工程师或者众包人员标注。 模型开发 模型的开发与调试需要算法工程师参与，通常使用Python脚本或Jupyter Notebook。常用框架有Tensorflow3 / PyTorch4。开发人员更注重IDE的便捷性，而小微开发者对算力价格更敏感。 模型训练 模型调试完成后需要大规模训练，通常会调整超参同时启动多个训练任务，这需要大量的算力，通常需要使用分布式训练系统。常用框架有PS（Parameter Server） / All Reduce / Horovod。 同时，训练任务的实时参数需要及时(比如通过Tensorboard)反馈给算法工程师，以便调整训练参数。 模型部署 模型训练完成后，挑选精度/计算量符合业务需求的模型优化部署。模型部署系统需要支持多种标准模型，并且提供后端服务直接调用的接口，注重高吞吐、低延时。常用框架有Tensorflow Serving / TorchServe / Neuropod。 在模型部署前，可以有针对地优化模型，以达到提高计算效率，降低推理延迟，节约计算成本等目的。 模型开发人员在首次开发新模型的过程中，需要手动完成模型的训练和部署。之后可以构建自动化训练部署的流水线（蓝色循环），定期或按特定条件触发。 我的经历是从后端硬件加速器入门，逐步扩展到后端硬件软件栈，模型推理优化，到机器学习系统的架构与产品。从下一节开始，我会逐一展开讨论。 [1] 机器学习 [2] ETL [3] Tensorflow [4] PyTorch","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"}]},{"title":"如何掌握复杂事物","slug":"think/complexity","date":"2020-04-18T10:23:19.000Z","updated":"2021-01-20T15:53:42.934Z","comments":true,"path":"think/complexity/","link":"","permalink":"https://blog.kleon.space/think/complexity/","excerpt":"理论往往是优雅简洁的，但是现实一定是复杂的。为了解决现实问题的理论通常会愈发复杂，解决方案可能会最终超越复杂，变成混乱。","text":"理论往往是优雅简洁的，但是现实一定是复杂的。为了解决现实问题的理论通常会愈发复杂，解决方案可能会最终超越复杂，变成混乱。 定性 究竟该怎么定义复杂？ 复杂在不同人看来可能很不相同。 正在背诵乘法口诀的小学生可能觉得复杂，正在理解理论物理公式的大学生也可能觉得复杂； 正在计算购物节满减凑单优惠的剁手党可能觉得复杂，正在试图理解经手多人外包的代码的程序员可能觉得复杂。 大学生看小学生的乘法口诀觉得小菜一碟，而程序员可能不一定能理解满减规则。 由此看来，复杂性是由概念数量决定的，需要同时考虑的概念数量越多，就会感觉到越复杂。 并且，复杂性与一个人的知识积累以及抽象能力相关，小学生背诵乘法表需要记忆。 我们面对如毛线团般的事物无从下手，而经验丰富的专家则能抽丝剥茧，重新梳理编织得井井有条。 那么我们与专家的区别在哪里呢？ 答案就在知识储备上。 知识储备的一种理解是，如果把我们的思维比作工具箱，把碰到的问题比作奇形怪状的钉子，那么我们的工具箱中有多少种可选的锤子可以帮助我们敲钉子，就代表我们的知识储备的丰富程度。 同样是看到一粒凸起的钉子，普通人（未经过专业训练的人）只会觉得无从下手，光是想象自己赤手空拳跟钉子软碰硬时，就觉得疼得要命。而专家可能会掏出他惯用的锤子，敲敲打打，即使不能把钉子敲好，至少手中有锤，心里不慌。 反映到现实中来，普通人如果看到一件事的发生，可能不知从何理解，甚至陷入对复杂性的恐慌之中，大脑一片空白。不过普通人绝不止这一种反应，比如还可能回忆起自己一知半解、道听途说来的概念，强行编造个因果关系。 如果我们能把现实简化，抽象出概念，建立模型，则能帮我们更好的理解事情，从而找到可能的解决方法。 值得注意的是，工具箱思维也只是对知识储备的一种片面解释，如果眼里尽是钉子，那不得跟肉中刺一样疼吗？。 模型 神经生物学揭示了大脑复杂的物理结构，我们目前很难自底向上地完全解释大脑的运作方式，不过我们在不同层次上建立了几种解释模型。 一种是基于信号测量的脑区划分方法（需要准确名称），这种方法通过实验测量大脑不同区域的活跃程度与行为之间的关系。 另一种是更抽象的概念模型，用行为来验证模型的适用范围。 认知心理学中就有一个“工作记忆”的模型。模型由长期记忆和短期记忆构成，工作记忆属于短期记忆的一种，相当于思考的工作台，而长期记忆相当于记忆的仓库，用于存储暂时用不到，但可能日后还有用的记忆。 每个人的工作记忆大小不同，每个概念占用一定的空间，如果概念数量太多以至于超过了工作台的最大容限，思考将变得困难。 我们可以把外界中的概念与工作记忆“交换”，比如用纸或者白板写下来，但因为高速的“内存”和低速的“外存”直接需要不断交换信息，思考的速度将下降。 另一个模型是关于快慢系统的：我们的大脑可以简化成两种思考系统，一种是系统1，通常是迅速的，潜意识的，反射式的，不由自主的；另一种是系统2，通常是缓慢的，费力的，需要意识参与的，可以控制的。 涉及的所谓“肌肉记忆”的技能，以及需要快速反应的“机械知识”都可以通过反复练习固化下来，直接经由系统1做出反应。而学习、分析、推理、想象等行为则需要系统2参与，这时影响思考效率的就包括同时处理的概念数量了。 我们感受到的复杂通常是系统2的产物，我们不会觉得辨认普通地辨认猫或者其他常见动物是件复杂的事。 这通常在一瞬间发生，当你看到一只典型的“猫”样的物体时，猫这个概念几乎不可阻挡的进入脑中，这也是系统1的特性，它是基于神经系统的。 至于接下来的处理步骤就取决于你当时的状态，如果你的注意力集中在别的事情上，那么这个概念也只会转瞬即逝，几乎不会有机会进入长期记忆。 如果你需要仔细辨认猫的种类，或者确认那个“猫”样的物体究竟是不是猫时，则可能需要系统2的参与。 如果你对猫的特征有过深入研究，并且熟悉它们，那么同样猫的品种就已经在系统1的自动处理流水线中处理完成提交到你的脑子里了。 而如果你刚刚开始学习辨认猫的品种，捧着一本手册努力查阅或者听一旁的朋友涛涛不觉地解释，就很有可能感受到复杂，并且经历被信息淹没的感觉。 对于你而言，那些非常细节的概念之间毫无联系，光是努力维持那些概念在脑子里就十分费力了，然而实际上那些概念刚从系统1的视觉或者听觉提交上来没多久，就直接被丢弃了。 你得承认，复杂超过一定程度，人就几乎不能理解了。 方法 我们利用了几种模型来帮助我们分析如何处理复杂性这个问题，这些并不是唯一的解释方法。不过这些模型对于我们的提出解决方法已经有足够的启发了。 根据我们的模型，我们该如何提高处理复杂的能力呢？缩减工作记忆中的概念数量以减少甚至避免和“外界存储”的交换，提高工作记忆的容量以容纳更多的概念数量。 那么如何缩减概念数量呢？我们可以对概念进行分类、抽象、归纳、封装、近似、封装。 分类 物以类聚，我们可以把相似的概念分为一类——具有共同特征，或者在时空上接近。这样，我们只需按类别考虑概念。 许多苹果、梨、香蕉、西瓜、西红柿、黄瓜、冬瓜、南瓜混在一起。如果我们分别考虑每一个物体，由于物体数量很多，就很难同时考虑。如果我们先按品种先分类，就只需要考虑8堆果蔬，一共8个概念。进一步，我们再按能不能带皮吃分类，则只需要考虑2种概念：能带皮吃和不能带皮的。在此基础上讨论理解，则比同时考虑多个具体概念容易得多。 当然分类的方法不止一种，比如按颜色分类、按大小分类，分类的具体原则要根据具体问题决定。适应问题的合理分类，能帮助我们更好的简化模型，梳理思路。 抽象 抽象是指把多种概念的共同特征提取出来，从具体的事物体中抽象出概念。比如考虑上面提到的果蔬混合物，在分类的基础上，我们可以提取出水果和蔬菜的概念，这样就能在一般性地研究水果或者蔬菜的特性。 有时候，抽象所提取的共同特征太少，导致可以被归类的概念太多、太宽泛，这种抽象可能不一定适合我们想要考虑的问题。我们可以多层抽象，比如将共同特征的要求放严一些，这样抽象得到的概念类别也就更多并且具体些。 合理的抽象层次，对模型的组织方式有很大影响。 归纳 苹果是甜的，梨也是甜的，西瓜是水果，那么西瓜是甜的吗？我们可以从苹果和梨的特性上尝试归纳出水果是甜的这个特征，以此为假设前提，并通过演绎推广到同样是水果的西瓜上，可以得出西瓜也是甜的的结论。这就是归纳-演绎的常见用法。 不过，要小心掉进逻辑的陷阱！归纳-演绎的结构推导出的结论正确，必须建立在归纳的结论正确之上，归纳的结论正确又必须建立在引用事实的正确之上。 有的人把假设当事实，自然会归纳得出荒谬的结论；有的人不看具体场景，只觉得要解决的问题与归纳的模式有几分相似便套用模板，自然错到离谱，在考试上俗称套公式。 封装 封装是将复杂而紧密联系的概念当成一个整体，也就是当成所谓的黑盒，在考虑更高层次的问题时，仅考虑这个整体与外界的互动，而不必关心它的内部细节。 黑盒的概念应用很广。在研究大脑和行为的关系时，就有人通过人接受的刺激和做出的行为来为黑盒建立模型，这种模型是概念上的。在生活上，我们经常也不自觉结合“常识”的给身边的“黑盒”建立模型，比如在炎炎夏日我们进入室内打开空调时，可能会把空调调到最低温度，而不是最适宜温度，这里就隐含了一个我们给空调这个黑盒建立的模型——设定的温度与当前温度温差越大，空调工作的功率越高，然而这不是空调的真实模型。 因此，封装并使用简化的黑盒可以缩减概念数量，但是要注意简化后的模型能在多大程度上还原之前的解释，是否符合需要。 分治 有时候，许多概念一拥而上，着实令人头疼。但在开始分析之前，先判断是不是这么多概念都是需要同时思考的，是不是可以分而治之。 分而治之的思想自古有之，如果我们面对大问题难以入手，那么就化整为零，再逐个击破。同样，面对繁多的概念，如果他们可以分解成更小的概念组，再分别考虑，那么一次要处理的概念数量就能减少不少。 近似 近似用哲学的话来说就是：抓住主要矛盾。 近似在工程上经常使用。比如我们通过公式推导出了一串复杂的解，我们通常会在误差允许的范围内对解做近似，忽略权重低的项，保留权重高的项。在简化了解的同时在很大程度上保证结论的有效性，并且这让我们能对更有影响力的变量给予更多的注意力。放在实际应用中，我们就能集中资源攻克主要问题。 同样，需要注意的是，不要忘记模型的公式本身是对现实的近似，答案本身是对精确解的解释，而如果使用计算机求解，同样要面对有效数字的近似问题。现实在层层近似之后通过仿真展示在我们面前，然而仿真不是真，也不是现实。 化简概念也是类似，把细枝末节剔除出去后，剩下重要的概念也应该不会太多。 关于如何扩容，工作记忆这个模型对应着物理的神经系统，想要扩容十分困难。有许多号称可以提高记忆力的方法，其实是也通过减少概念数量。比如，通过谐音的方法将需要记忆的概念连成一个故事或者组成一幅画面，概念之间不再毫无关联，而是形成了一个可以自动播放的整体，回忆时再通过机械记忆住的映射规则（回忆一下小时候背乘法口诀表或者中学背元素周期表）反向解释即可。 应用 以编程为例，开始学习时，我们更注意程序语言的语法，通过一些简单的例子体会语言的特性。这时语法上的每一个特征对我们来说都是一个独立的概念，我们需要不断回顾语法规则来加载概念到工作记忆。此时十分费力，也就是入门的阶段。 我们在从外界反复使用这些新概念的同时，我们的大脑会根据“重复=重要”的原则把这些概念放入长期记忆中，并且通过反复练习提高熟练度可以提高概念从长期记忆中提取的速度。 在反复练习之中，我们会发现不必过于关注语法本身，而可以总结出基本的使用模式。 我们可以抽象出控制结构、循环结构这样的基本结构，明白了可以使用函数来把一系列紧密相关的实现封装起来。我们在阅读别人的代码时，可以不必关注实现细节，而是通过主要函数的调用关系理解整个模块的功能。 在这种自底向上的抽象过程中，可能会引入新的编程模型，新的功能模块，新的独立系统。 得益于此，我们可以在高抽象层次上设计系统。我们有能力把握有限的子系统数量，如果子系统的数量过多，可以进一步抽象，保持整体的概念数量可控。","categories":[{"name":"学习","slug":"学习","permalink":"https://blog.kleon.space/categories/学习/"}],"tags":[{"name":"学习","slug":"学习","permalink":"https://blog.kleon.space/tags/学习/"}]},{"title":"如何快速入门新领域","slug":"think/something_new","date":"2020-03-02T14:08:35.000Z","updated":"2021-01-20T15:53:42.957Z","comments":true,"path":"think/something_new/","link":"","permalink":"https://blog.kleon.space/think/something_new/","excerpt":"从零开始学前端（移动端）的过程记录，以及走神想到的关于学习新领域的思考。","text":"从零开始学前端（移动端）的过程记录，以及走神想到的关于学习新领域的思考。 缘起 互联网上的垃圾越来越多，各个大厂的生态也趋于闭合。即使仍然还有人在免费提供优质的内容，Google的爬虫也无能为力。 现在大行其道的推荐算法，它或许可以通过我的注意力停留时长判断出我的兴趣所在，但在一人一票的制度下，很难为我们挖掘真正沉淀了价值的东西。 信息就像一次性纸巾，在被人擦过鼻涕后便只能扔掉，而在一些激励的鼓励下，有不少人在全天无休的制造着这些白色垃圾。 这是我最初开始设想阡陌时的一个初衷：发掘高质量、长生命期的内容。 具体的设想暂且不展开，打造一片理想的数字世界需要技术的支持，而作为数字世界大门的就是前端。 我之前对于前端的了解很少，大概觉得网页就是前端。不过，在各行各业，“前端”都有所知，我之前做硬件开发时的前端便是Verilog HDL。 虽然对前端不太了解，但是我觉得计算机软件开发技术可能是所有应用技术里最适合自学入门的，不仅网上现成免费的书籍、课程多，而且得益于开源的兴起，实战项目也不少。 不过这些资源散落在互联网的汪洋大海中，并且其中有一大部分处于公海之外的私人水域中。除此之外，资源的质量如何，难度是否合适，应该何时使用，却少有说明。 为数不多的路线地图（Roadmap）也只是笼统而论，还没入门的小白用户看起来自然也就一头雾水，等到七拼八凑，走过了不少弯路，才懵懵懂懂有了些印象。 这些是我个人在学习过程中的体会，姑且先推广开来，假设和我有相似遭遇的大有人在吧。 据此至少可以提出三个需求： 第一，我希望有一份详细的路线地图，可以让我对需要学习的领域形成大致的“方向感”； 第二，我希望路线地图拆分成里程碑（Milestone），把一个大目标拆分成小目标，逐个击破，在及时反馈之中增强信心； 第三，我希望学习之路上，有人通行，亦师亦友，汇总常见问题，甚至可以做成专家系统，节约入门者和指导者的时间。 评估 首先，分析我的位置： 掌握Python、C++等编程语言。 了解计算机网络基本常识。 熟悉搜索引擎的使用方法🤣。 其次，权衡我的目标： 网页端、移动端还是桌面端？ 这其实是在考虑这三种平台优势是什么。 网页端适合电脑使用，对实时性要求不高，合适生产力模式下使用。 移动端便携性好，普及程度广，合适随身模式使用，但可能被娱乐模式干扰，优先级较高。 桌面端适合电脑使用，有更强的性能与定制性。 现有主流框架有哪些？是否有跨平台框架？ 网页端有React、Angular、Vue，移动端有Android/iOS原生，桌面端各个系统各式各样。跨平台的方案有React Native、Electron、Flutter。 各个框架的优势是什么？ 对比主要从代码复用性和社区成熟度上考虑。代码复用性包括跨平台代码复用，可用第三方库。社区成熟度包括文档、教程的完善程度，项目活跃度，是否仍在积极维护。 框架的选择也是见仁见智，JS框架在性能上可能不如意，移动原生开发需要的学习量和工作量太过巨大，跨平台方案不太成熟。评估之后还是选择移动端切入，日后可以补充JS框架做网页端。 这里的问题是，我从网上获得的评论都具有主观性，对前端几乎为0的了解也导致我没办法做出合理判断。再者，技术只是实现目标的手段。总之，先淌一淌这浑水再做决定。 路线 学习因目标不同，知识类型不同，有许多不同的方法，从大的方向上可以分为三个阶段。 直接上手： 这适合早期入门。在老师的指导下或参考有效的资料，形成对知识的感性认识。常见的学习模式是：你不要问那么多为什么，照做就是。很多时候，通过这种方法，就能大致看懂别人的成果并简单应用了。并且，通过上手-反馈循环，可以积累信心。虽然大多数知识的入门都可以使用这个方法，但通过这个方法学到的知识不系统，存在逻辑缺失，在学习的时候可能会在心里存下不少疑问。比如在出现复杂问题时，可能无法解释，只能诉诸玄学。这就是缺乏系统化、整体化的认知的结果。 系统学习： 这适合中期进阶。之所以能系统学习，是因为知识体系相对稳定，有大量前人总结的体系模型可以参考。系统化学习通常来说要明确学习领域和起点，预先给出公理或者假设，在大家都明确了以下所有内容均基于以上公理或者假设——也就是明确了知识体系的适用范围之后，开始引出推论和模型，并且通过严格的逻辑证明推论成立，或者通过模型解释实际案例表示模型有效。经过系统学习，基本上能对大部分问题套用模型分析解释，解决问题，达成目标，有了更强的定制能力。在系统学习的过程中，可以解释之前遇到的不少疑惑，但系统学习需要警惕绝对知识论，需要明确知识是在一定假设上的理想模型，并不是绝对真理。如果批判性地思考学习，心中的疑问可能只增不减，但这并不是一件完全的坏事，这说明对知识有了更全面深刻的认识。所谓知道的越多，才发现不知道得更多。在系统学习过程中，有人不能接受完全接受模型，而是会对为什么提出模型产生疑问，因为知识体系是经过无数人试错后的总结，而了解知识诞生的历史会让我们学习知识的逻辑更加顺畅，因为建立知识体系的历史是探索与偶然的历史。系统学习并不是学习的终点，已知的模型在面对新的问题的时候很可能会失效，这时候就需要科学探究。 科学探究： 这适合后期探索。之所以带上科学二字，是因为需要使用科学研究方法，并且得出的知识模型可证伪。不可证伪的知识通常是万能解释的，比如很多伪心理学总能用一套看似有道理的解释套用到所有情况上，并且没法提出切实可行、可重复的解决方案。使用这种方法需要具备丰富的想象力和严谨的逻辑。丰富的想象力要求有能力补足未知的细节，建立知识模型，并设计证明方法，对结果有预期。严谨的逻辑需要知识模型具有可解释性，以前人已有的知识或者可理解的假设作为起点，构建一条完整的逻辑链路。科学探究能构建理论解释之前的一部分疑问，但却有可能调入疑问的黑洞之中，不能说科学的尽头是神学，但是面对自然，仍然要抱有敬畏之心。虽然我们有了一些用来抵御未知的知识，但也要意识到我们这片知识空间的狭小和脆弱，在它之外的是漫无边际、比黑更黑的未知。 希望以上这些可能对处于不同学习阶段的人有点启发，当然这并不是说所有的学习过程都要以第科学探究为最终目标，需要根据自己的实际的目标和所处的位置选择合适的方法。比如纯粹建立在逻辑上的知识可能就不能直接上手，只能系统学习了。又比如应用性强的知识适合直接上手，即所谓自学成才，可能身经百战，但不知其然，但也可以系统学习，即所谓科班出身，可能头头是道，但纸上谈兵。 值得警惕的是那些打着科学旗号兜售自己的“万灵药”的人。 回到前端入门上来，因为是应用性很强的知识，所以我选择直接上手，需要深度定制或者遇到难解的问题，再回头补习。 上手 搜了些路线图，有些前人的路可以参考，但完全照搬可能有些拘束，根据自身情况借鉴即可。 第一步：环境准备。这是几乎学习新编程语言必备的，不同教程大同小异，如果社区成熟，按照文档做一遍就行。一些特殊的网络问题，需要想办法解决，同样成熟的社区也会有相应的解决方案。 第二步：玩具教程，这通常也是出现在社区文档中的。玩具，又称小玩意儿，通过这些简单的小玩意儿，能大概有个端到端的成果可以展示了。可不要到此就说自己精通了哦。 关于语法什么的，如果有相关的编程基础（概念都是相通的，语法存在差异），大概也能猜个大差不差，如果只知道面向对象，而学习函数式可能理解起来就有点费劲，不妨从基础概念开始理解。 第三步：了解常用组件，通过但不限于高质量的知识沉淀网站，风靡全球的代码托管网站获得基本了解，如果能找到好的起步教程就很好。一个理想的教程符合直接上手的原则，在上手过程中，介绍了常用的组件、使用方式，也就是“套路”。如果这一步有概念缺失，需要自行回溯学习前置知识，只需要回溯到连蒙带猜理解个大差不差的程度就可以回来继续教程了。 第四步：好了，你已经知道了1+1=2，下面我们来算个不定积分。这是不少人可能遇到的情况，教程也能看了七七八八，却发现离自己想实现的目标还有不小的差距。 这个差距通常是几方面方面引起的：不知道某项功能怎么实现，需要定制第三方库，需要自己完全实现。 我们需要逐个击破： 不知道用什么轮子：搜索学习更复杂的教程，最好包含自己想实现的功能，或者其中一部分。 有轮子但需要调整：回溯学习第三方库相关知识，尝试修改并观察结果，理解基本原理直到可以上手修改。 找不到轮子：回溯学习所需的知识，自底向上，逐步搭建。 其实到这一步，也算淌了些水，我们可以先停一停，需要回到评估这一步。如果轮子缺失严重——这通常意味着社区还不够成熟以及巨大的工作量，可能需要考虑使用别的方案，这很有可能意味着评估阶段的工作不足，或者领域太新。如果重新评估后发现其他方案更完善，那么应该切换方案。可能有人会抱怨以上四步白白浪费了时间，其实可以从两方面来考虑。其一，这四步消耗的时间是沉没成本，没办法收回，最合理的做法是忽视它，评估继续这个方案所需要的投入以及其他方案从0开始的投入哪个更多。其二，面对一个问题的多种解决方案，通常在回溯之后的知识上是相通的、可重用的，时间也不是完全浪费的。 我学到这里，基本上达到了可以使用轮子搭建自己想要的功能的地步。向后就需要根据设计的功能反复使用第四步的三个方法，直到满足我们的设计，或者修改设计降低实现复杂度。 后续 我在这篇文章里，没有重点讲具体的知识点，而是想通过这次的学习实践归纳一般的方法。当然，抽象得足够高的规律都可能变成“万灵药”的危险。 前端先学了一点皮毛，先暂且用着，还要补些网页前端、后端、设计、认知科学方面的知识。","categories":[{"name":"学习","slug":"学习","permalink":"https://blog.kleon.space/categories/学习/"}],"tags":[{"name":"学习","slug":"学习","permalink":"https://blog.kleon.space/tags/学习/"}]}]}