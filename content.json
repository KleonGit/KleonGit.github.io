{"meta":{"title":"KLEON","subtitle":null,"description":"Think About The Big Map","author":"Kleon","url":"https://blog.kleon.space"},"pages":[{"title":"","date":"2021-03-04T07:22:23.195Z","updated":"2021-03-04T07:22:23.195Z","comments":true,"path":"googlebe23cb0bc55fc412.html","permalink":"https://blog.kleon.space/googlebe23cb0bc55fc412.html","excerpt":"","text":"google-site-verification: googlebe23cb0bc55fc412.html"},{"title":"","date":"2021-03-04T07:22:23.195Z","updated":"2021-03-04T07:22:23.195Z","comments":true,"path":"about/index.html","permalink":"https://blog.kleon.space/about/index.html","excerpt":"","text":"有意思"},{"title":"分类","date":"2021-03-04T07:22:23.195Z","updated":"2021-03-04T07:22:23.195Z","comments":true,"path":"categories/index.html","permalink":"https://blog.kleon.space/categories/index.html","excerpt":"","text":"分类浏览。"},{"title":"标签","date":"2021-03-04T07:22:23.211Z","updated":"2021-03-04T07:22:23.211Z","comments":true,"path":"tags/index.html","permalink":"https://blog.kleon.space/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kubernetes实战 1 - 存储系统","slug":"k8s/in-action/1-storage-on-k8s","date":"2021-02-26T14:00:01.000Z","updated":"2021-03-04T07:22:23.195Z","comments":true,"path":"k8s/in-action/1-storage-on-k8s/","link":"","permalink":"https://blog.kleon.space/k8s/in-action/1-storage-on-k8s/","excerpt":"云厂商的k8s集群通常提供默认StorageClass，可直接创建磁盘资源。私有k8s集群需要部署存储系统支持PVC的创建。","text":"云厂商的k8s集群通常提供默认StorageClass，可直接创建磁盘资源。私有k8s集群需要部署存储系统支持PVC的创建。 需要Kubernetes基本知识 本地存储 local-path可以用local-path-storage部署基于本地存储的StorageClass。方便灵活，但没有冗余存储，如果Node节点挂掉，相关的PV都会丢失，需要及时备份。 1kubectl apply -f https://github.com/rancher/local-path-provisioner/blob/master/deploy/local-path-storage.yaml 对象存储 MinIOMinIO支持S3，可通过Helm Chart部署服务。 默认部署单节点服务。1helm install stable/minio 分布式服务通过纠删码(EC，Erasure Code)保证高可靠性，需要多节点集群，准备可用的动态存储(longhorn、rook、openebs等)。1helm install --set mode=distributed stable/minio 可以使用MinIO Operator管理租户。 分布式文件系统/对象存储 Rook-Ceph 需要集群配置CSI，如果是云上集群，不要申请FlexVolume。 Ceph支持分布式块存储、文件系统以及对象存储。 安装Cephfs)支持分布式文件系统。 123456789101112131415git clone --single-branch --branch release-1.5 https://github.com/rook/rookcd rook/cluster/examples/kubernetes/cephkubectl apply -f common.yamlkubectl apply -f crds.yamlkubectl apply -f operator.yaml# Wait until operator is runningkubectl apply -f cluster.yaml# Install Cephfskubectl apply -f filesystem.yaml# Install StorageClass rook-cephfskubectl apply -f csi/cephfs/storageclass.yaml# Mark rook-cephfs as the default storage classkubectl annotate sc rook-cephfs storageclass.kubernetes.io/is-default-class=\"true\" 安装ceph-rgw支持S3对象存储 123456789kubectl apply -f object.yamlkubectl apply -f storageclass-bucket-delete.yamlkubectl apply -f object-bucket-claim-delete.yamlkubectl get svc rook-ceph-rgw-my-store -n rook-ceph# Get Bucket Configexport AWS_HOST=$(kubectl -n default get cm ceph-delete-bucket -o yaml | grep BUCKET_HOST | awk '&#123;print $2&#125;')export AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '&#123;print $2&#125;' | base64 --decode)export AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '&#123;print $2&#125;' | base64 --decode) 使用s3cmd检查存储情况，集群内可通过Hostrook-ceph-rgw-my-store.rook-ceph.svc访问bucket，可参考文章。 安装ceph-dashboard12345678910# Either Use HTTP Port 7000# yq w -i cluster.yaml spec.dashboard.ssl false# kubectl apply -f dashboard-external-http.yaml# Or Use HTTPS Port 8443kubectl apply -f dashboard-external-http.yaml# Default Username: admin# Default Password:kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\"&#123;['data']['password']&#125;\" | base64 --decode &amp;&amp; echo","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.kleon.space/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.kleon.space/tags/kubernetes/"}]},{"title":"机器学习系统 11 - 后端硬件系统","slug":"ai/ml-system/hardware-backend/11-system","date":"2021-02-04T17:28:01.000Z","updated":"2021-02-04T17:28:01.000Z","comments":true,"path":"ai/ml-system/hardware-backend/11-system/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/11-system/","excerpt":"现代的服务系统已经变得庞大复杂，硬件后端需要与服务系统整合，才能在整个pipeline中发挥作用。相比于底层更关注局部性能，服务系统更关心端到端延迟，并发量和硬件利用率，性能稳定性，服务可用率等指标。","text":"现代的服务系统已经变得庞大复杂，硬件后端需要与服务系统整合，才能在整个pipeline中发挥作用。相比于底层更关注局部性能，服务系统更关心端到端延迟，并发量和硬件利用率，性能稳定性，服务可用率等指标。 Serving后端硬件接入框架后，基本就可以依赖框架的Serving能力对外输出了。比如Tensorflow Serving、Torch Serve。NVIDIA也凭借硬件优势，提供了支持GPU虚拟化的Serving方案。 Serving框架相比于Training框架可以去除大部分逻辑，保留基本的计算调度和算子实现功能。框架通常基于Training框架的计算核心，封装模型加载，通信协议处理等逻辑。 框架基本的性能取决于计算核心，但更关注并发性能，使用HTTP/RPC等协议，RPC使用gRPC（Google）或Thrift（Facebook）等RPC框架。 通常可以进一步优化训练后的模型，使用比如子图融合，编译优化，模型压缩等方法，提高模型的推理性能，达到降低延迟，提高吞吐，降低成本等目的。 容器化与隔离随着容器化的流行，带来了无状态确保无副作用，开发和生产环境一致，环境可复现等诸多好处。构建一个容器化的Serving环境已经是基本操作。 容器和虚拟机不同，容器更轻量，也意味着隔离的不彻底。尤其是在多租户使用场景下，驱动文件可以直接mount，驱动层就需要考虑严格的内存权限设计。 而虚拟机层面需要一些设备虚拟化的工作，比如FPGA的虚拟化，通常这部分由云厂商提供。用户可以直接购买的云主机就是一台虚拟机，按资源量将一台物理机内存CPU拆分售卖，称为实例。如果需要物理机可以申请bare metal机器，但对于中小应用开发者，出于成本考虑，可能不会直接购买物理机。 虚拟机在一定程度上确保了CPU核和内存的隔离，但在Cache和内存带宽上的隔离比较困难。尤其是目前CPU通常使用HyperThread，相当于一个物理核，对应多个逻辑核（通常是两个）。申请小型实例——最极端情况是1核——就可能会遇到和其他实例共享物理核，影响性能。除此之外，多核CPU普遍使用NUMA结构，跨NUMA访问会对性能有较大影响。 云厂商提供的虚拟机的隔离性较高，但对于一个用户来说，不需要每个容器都通过实例隔离，而是需要和其他用户的容器隔离。一般通过云厂商提供的网络（VPC1）隔离，配置安全组限制访问权限。同一个VPC内的实例上可以放心部署用户的容器，容器的安全性由用户保证。在多租户场景，对隔离有较高要求的情况下，就会使用这种硬多租方案，申请实例，并与用户VPC连通。 Serving服务在部署时，通常需要注意容器的隔离，除了考虑多用户的安全性外，更多的是资源的隔离性，比如通过绑核来控制CPU的独占性，这在许多CPU推理优化方法中是很关键的。 集群化与弹性随着CPU性能触顶，从提升频率，到多核，再到分布式，对计算性能的需求推动了集群的大规模使用。其中用于自动调度编排的系统Kubernetes2已经逐步称为集群开发的事实性标准，也称为云原生开发的基本“操作系统”。类比于操作系统，应用对单台机器资源的管理调度（内存，CPU核，进程，线程等）等变成了对更宏观的云上资源（计算资源，数据库，网关等）的管理调度。 Serving服务同样可以融入这套系统。Serving服务注重的主要是以下几点： 服务可用性 这是排在第一位的，Serving服务是需要持续调用的，如果出现不可用，则可能造成巨大的业务损失。使用集群也有这方面的考虑，单一节点失效并不影响整体服务。 性能稳定性 在服务持续可用的情况下，服务调用的性能指标随请求量增减的波动不能过大，轻则链路延迟较高，业务体验下降，极端情况是超时，链路崩溃，业务不可用。 通常通过资源弹性满足性能稳定性，对于调用变化较缓慢的场景是适合的，资源随调用量，CPU水位（利用率），内存使用量等指标增减。 但对于瞬时调用量洪峰，则不可行，服务会出现短时不可用的情况，通常需要提前规划资源，灰度压测，保证在最高瞬时并发下服务的可用性。 资源弹性 对很多应用来说，实时调用存在明显的波峰波谷，比如在晚间最高，凌晨最低，调用规模可能差数倍，如果提前购置资源拉高成本，Serving服务应提供资源弹性，满足用户在成本和性能稳定性上的需求。 资源超卖 对于Serving服务提供者来说，如果使用自有集群，资源的持有成本是很高的，通常资源划分为半托管（代持用户资源），和全托管（自持资源）。 半托管下，用户完全为资源付费，通常是对性能指标极为敏感的，资源水位较低。而全托管下，由于不需要完全持有资源，用户成本可以降低，因此对性能指标没有那么敏感。 因此服务提供商会超卖，将多个服务调度到相同资源上，进而提高资源水位，节约成本。服务提供商通常会使用虚拟化技术，模型性能优化等方法，维持性能，不至于大幅影响体验。 在离线混部 Serving服务可以分为在线调用和离线调用。在线调用对延迟敏感，为保证响应延迟，通常资源水位较低。离线调用对单次调用延迟不敏感，要求总体吞吐，资源利用较充分。 因而两者的目标不同，通常会分池，划为在线和离线集群，互不影响。而出于成本考虑，在线调用有波峰波谷，完全可以在波谷时利用闲置的资源，一个是利用弹性，另一个就是混合部署。 混部对资源的隔离性的要求更高，但总体上可以降低运维成本，避免分池和资源调度的麻烦。 接口统一Serving框架众多，最简单的可以用Flask包一个Python脚本就能服务。但对于Serving服务使用者来说，统一调用接口是一个必然趋势。可以避免切换模型训练框架对业务后端代码的影响。 比如Uber的Neuropod，AWS的SageMaker，Kubeflow的KfServing都在一定程度上制定了调用接口标准。 后端硬件小结机器学习模型服务只是整体数据分析链路中的一小部分，整体趋势是希望和具体框架解耦，并且自动化。因而机器学习系统通常和数据分析系统紧密联系，前端对接数据仓库，数据湖，后端对接业务后端，比如推荐系统，风控系统，BI（商业分析）系统等。 机器学习系统致力于更快捷，更自动化地构建部署机器学习模型。业务数据源源不断地产生，经过ETL汇入数据仓库，算法工程师进行特征工程，通过训练系统构建迭代部署机器学习模型，并固定为流水管线，通过AB Test系统上线验证效果并反馈迭代模型。 市场的粗放式扩张不再，转而对自己圈到的一亩三分地精耕细作。工程师们日复一日试图从庞大的数据中归纳出概率分布，去拟合稍纵即逝的注意力和兴趣的真实分布，优化漏斗变现的效率。 机器学习本身就是为了提高效率，降低成本，硬件后端的优化创新在整个成本效率优化链路中只占一个很小的点，市场枯荣会随本身ROI的评估而不断改变。 算法的更迭成本比起硬件小的多，当初认为不可解的循环迭代依赖也被替换成了符合GPU并行计算要求Transformer，硬件在定制应用上追着算法跑可能就像夸父逐日。 通用性和专用性的交替更迭，便随着经济周期波动下的新生与整合。 Xlinx被AMD收购，Nvidia吞并了ARM，巨头们纷纷试水GPU为破除Nvidia垄断，Intel在x86上三年又三年的缝缝补补，ARM和RISC-V已经NPU加入指令拓展。 FPGA只踏出了机器学习硬件后端的一小步，NPU已经融入了主流芯片，GPGPU的竞赛正如火如荼，CPU的指令集市场份额也在悄然演变。 谁能成为终局硬件？ [1] Virtual private cloud [2] Kubernetes","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 10 - 后端硬件框架接入","slug":"ai/ml-system/hardware-backend/10-framework-integration","date":"2021-02-03T17:28:01.000Z","updated":"2021-02-03T17:28:01.000Z","comments":true,"path":"ai/ml-system/hardware-backend/10-framework-integration/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/10-framework-integration/","excerpt":"在开源背景下，框架在社区的加持下不断迭代，后端硬件通常不能cover所有应用场景，需要接入框架借用框架的能力。那什么样的实现最合适，又会有哪些问题？","text":"在开源背景下，框架在社区的加持下不断迭代，后端硬件通常不能cover所有应用场景，需要接入框架借用框架的能力。那什么样的实现最合适，又会有哪些问题？ 按需求不同，可以有多种方法： 不接入 实际上还是借用了框架的模型定义。如果整个模型都能被支持，并且对性能有较高的要求，场景定制，那么可以不接入框架，比如TensorRT可以独立运行。 后端软件栈可能需要不小的开发工作支持长尾算子，达到全图覆盖，对于GPGPU这种适用面极广的后端硬件是可行的，TensorRT也标榜可以达到最高性能。 算子内接入 最直接的接入方法，利用后端软件接口直接在算子内实现等价逻辑，不用关心模型长什么样，而且不管支持了多少算子都能运行，提高覆盖率只是一个体力活。 这种方法针对计算密集型算子优化明显，尤其是当密集计算占比较高，其他计算和通信几乎不影响系统整体性能的时候。比如基于CNN的一系列模型，都可以用这种简单粗暴的方法先支持起来。 但这种方法的缺点也很明显，一旦计算较分散，每次执行算子都要做一次数据交换，对于图像类模型，带宽占用不低，会拖慢系统整体性能。 而如果为了最大化硬件利用率——计算时间占总运行时间的比例——加入指令队列，不做密集算子的interleaving，可能会引入较高的性能抖动。 除此之外，单算子性能最优并不必然意味着系统性能最优，系统最优通常需要在更高层面协调优化。 图层面接入 稍微复杂，借用了框架的算子调度能力。主要工作在如何从现有计算图中划分出适合的子图。如果子图过小，那么设备context切换（内存交换，通信）的开销就可能过大。 有时候会分出过多子图，但覆盖率很高，可能是子图间的特殊算子。 如果是训练算子，通常不会占用主计算通路，可以提前使用清理工具去除。 如果是Fusion等优化引入的算子，就可能需要“反编译”，把fuse起来的算子break down。 如果是控制结构切割了子图，需要从建模层面尽量消除控制结果，比如并行，展开，将控制结构放在生成代码而不是计算图里。 如果子图间不存在数据依赖，可以融合子图，减少通信开销。 上面提到的方法都需要根据实际情况取舍，因此，分图算法可能会引入更复杂的策略。比如在估计子图大小上，可以用最简单的个数阈值，也可以构建cost model。 Cost model可以根据shape和算子估算，也可以通过profling测量，后者相对于前者更自动化，并且可以支持运行时（动态）场景，而前者基本可以在编译时（静态）确定。 有时候分图算法过于粗暴，比如直接使用算子集，而后端硬件对结构有要求，则可能出错，需要有fallback机制，或者使用更灵活的模板匹配算法。 由于框架算子层接口可能随大版本更新变动，比如Tensorflow的Grappler（通常是因为protobuf版本问题），可能需要考虑兼容多版本，尽量解耦，复用代码。 编译层接入 为了用更通用，更自动化的方法优化能，减少手工优化的工作量，不少研究者提出借鉴编译器的思路，其中Tensorflow的XLA算是较早开始尝试的。 XLA1同样是在图层面接入的，也有一套分图算法，由于XLA使用的是编译，因此只要标记符合的算子即可。 XLA嵌在Tensorflow中，在运行时JIT执行，在分图后，符合条件的子图会开始编译流程，转换为HLO表示——更小的粗指令集合，如Softmax等算子可能被打散。 转换后的HLO执行一系列的设备无关优化，再交由后端设备Service处理，执行一系列设备相关优化，CPU可以通过LLVM Emitter发射为LLVM IR，交由LLVM编译为CPU后端指令。GPU等较通用设备可以复用LLVM Emitter结构，生成对应的指令。 编译后的指令会缓存在XLA的Compilation Cache中，根据Shape和Op计算Hash值存储索引。 在编译层接入，需要硬件具有一定的通用性，并且要在图层面实现设备相关的分图逻辑。虽然HLO缩小了算子集，但要求对编译后端有足够的了解，开发工作量不小。同样，图层面接口需要注意和版本解耦。 其他 接入后端的一个重要问题是线程冲突，需要通过全局线程池统一来解决，可以使用同一个线程池实现，或者是可控的线程数量。 编译优化之路总是吸引人的，通用自动化不重复也是人们不屑的追求。如果要全面支持各类框架，工作量也是不小的，通常会是社区为了推广框架，做不少示范性接入工作，测些数据，发些paper。大厂是否跟进，还是取决于最终收益。 不过现实是模型的根基并没有百花齐放，而是依靠一小撮人探索出的最高效的结构趋同了，因此大厂因靠不断累积手工优化经验，也已经能支持成熟可商业化的模型了。而各类编译方法面临的困境是无法在所有场景下大幅超越手工优化，主要是填补长尾空间。 历史积累的力量是强大的，不仅展示了持续规划迭代的惊人成果，同样意味着推到底层设施重来的阻力必然很大。 [1] XLA: Optimizing Compiler for Machine Learning","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 09 - 后端硬件编译器","slug":"ai/ml-system/hardware-backend/9-compiler","date":"2021-01-26T14:25:01.000Z","updated":"2021-01-26T14:25:01.000Z","comments":true,"path":"ai/ml-system/hardware-backend/9-compiler/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/9-compiler/","excerpt":"编译，广义上是将一种表达映射成另一种表达，维持功能不变。深度学习后端硬件的编译，是将DSL描述的计算图映射成后端硬件指令。","text":"编译，广义上是将一种表达映射成另一种表达，维持功能不变。深度学习后端硬件的编译，是将DSL描述的计算图映射成后端硬件指令。 传统编译器直接解析文本，通过词法与语法分析，构建AST1。之后在AST上执行一系列的优化，称为优化pipeline。 优化后的AST可以转换为IR（比如LLVM2 IR），由LLVM进一步优化，并生成后端代码。如果是添加新后端并且复用LLVM，需要实现对应Codegen逻辑。AST也可以不借助LLVM，直接编写codegen逻辑生成不同层级的code或指令。 随着深度学习模型越来越复杂，在生产中广泛使用，对计算性能和成本的需求日趋强烈，深度学习编译器在这样的背景下逐步发展壮大。 在深度学习编译器提出前，广泛使用的优化方法是手工fuse和手工codegen。但这样的劣势显而易见，模型中可能存在的子图pattern千奇百怪，并且各种自定义长尾算子层出不求，如果一直依靠手工优化，开发时间和人力成本都不太能令人接受。 因此，借鉴传统编译器的思路，通过有限的基础表达符号表示任意计算图结构，由编译器的codegen层统一做后端代码生成。 分类编译器根据编译发生时间的不同可以分为AOT3（Ahead Of Time）和JIT4（Just In Time）。 AOT编译通常用于standalone编译程序，由用户提前运行编译生成指令，并传输到运行设备上（非标设备/专用设备，比如ARM和定制NPU），通过runtime执行。 一次编译，到处运行，这允许AOT编译消耗更多的时间，探索更优结果，但也限制了灵活性。每当模型发生改动就需要触发一次编译行为，通常编译设备和运行设备是分立的，指令文件和runtime版本的同步会更复杂。 JIT在运行时条件触发编译，runtime和compiler捆绑，不存在版本不匹配的问题。JIT通常见于解释型程序，因为解释效率通常低于编译，通过运行前编译提高执行效率。 由于在运行时编译，编译时间相对运行时间必须要合理，这意味着不能引入过于复杂的优化逻辑，另外为了避免重复编译，可以加入cache（程序内/远程）提高效率。 前端编译器前端需要读取各个框架的模型文件，比如Tensorflow的SavedModel或者PyTorch的TorchScript。 也有试图将模型表达统一的工作出现，比如ONNX，试图在Caffe、PyTorch、MXNet等框架间构建转换桥梁。愿景很好，方便用户在不同框架迁移，不被框架锁死，但最终没有被大规模使用。 首先，Tensorflow出自Google，并不买账，ONNX是其他大公司的联盟产物，然而框架上只有PyTorch不断发展，和Tensorflow二分天下。 统一模型表达，还是在统一社区，也就是统一算子集。 Tensorflow曾凭借其绝对的垄断地位十分强势，后端软件栈都主动接入，接受Tensorflow的核心算子集为golden标准。然而不同框架的算子集相差很多，基本上只有最主流的算子可以无缝转换，换个复杂的模型很有可能就挂了。尤其是Tensorflow的计算图，控制结构十分复杂，连官方的优化工具都可能出现bug。如果模型定制程度高，不如直接改脚本来的快。 当然对大多数算法工程师来说，没有被框架锁定这件事，只有看哪个框架的生态的轮子能符合业务需求。 优化编译优化方法泛指一大类模型优化方法，比如Tensorflow自带的后端XLA，基于profiling搜索的TVM，基于整数线性规划的多面体方法等。 Tensorflow自带的前端计算图优化Grappler也可以算作编译器的优化功能。在图层面执行一般优化方法，比如CSE（公共子表达式消除），Constant Folding（常量折叠），去除无用节点（Dead Nodes）等。 对于硬件后端编译器，主要优化步骤如下： 硬件无关优化。 尽可能从子图中匹配更多符合硬件结构的pattern。 匹配可以转换为符合硬件结构pattern的子图，并实现转换逻辑（硬件相关优化），注意功能正确性。 尽量将匹配到的子图融合，使之最大化连通。 设定不连通子图最小阈值，防止主存和硬件内存的context switch开销过高，可以通过算子数量或者算力评估确定阈值，标记所有符合阈值的子图算子。 融合所有标记的子图，生成对应的模型IR或者硬件IR。 可以通过自定义Op的形式，将生成的IR序列化（作为Op参数），使用统一的Op（调用runtime）。 如果模型可以全量支持，则可以直接导出IR，使用runtime运行。 进一步的，编译器可以独立为TensorRT的形式，允许用户使用API直接构建运行，并且可以通过JIT codegen dispatch手工优化的code。 编译器相比于计算库更关注图层面优化，底层会协调计算库的不同实现使全局性能最优。 [1] Abstract syntax tree [2] The LLVM Compiler Infrastructure [3] Ahead-of-time compilation [4] Just-in-time compilation","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 08 - 后端硬件运行时","slug":"ai/ml-system/hardware-backend/8-runtime","date":"2021-01-25T22:17:34.000Z","updated":"2021-01-25T22:17:34.000Z","comments":true,"path":"ai/ml-system/hardware-backend/8-runtime/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/8-runtime/","excerpt":"运行时（Runtime）抽象出计算模型，提供更通用的接口，进一步屏蔽硬件细节。","text":"运行时（Runtime）抽象出计算模型，提供更通用的接口，进一步屏蔽硬件细节。 在深度学习硬件加速系统中，如果仅依赖驱动接口，业务层/编译层代码中仍然要编入硬件相关代码，需要管理设备，记录内存，整理数据等，耦合较重，迁移困难。 在驱动之上封装一层runtime有利于进一步提供代码复用程度，将硬件实现和上层应用完全解耦。 在后端硬件系统设计早期，很可能因为项目压力而倾向于把所有功能写在一起： 程序读入模型并统一转换为内部定义的模型标记格式。 接着程序根据标记构建计算图，优化使得计算图的pattern符合硬件需求。 随后程序根据硬件结构执行算子融合，并生成对应的硬件指令。 此时，应用层、编译层、运行时完全耦合，专为应用定制设计，模型结构也可能硬编码在代码里。 这样的设计会带来不少问题： 整个程序更重，基本上要保证能执行全部算子，否则需要将框架反向接入，使用框架的算子覆盖长尾逻辑。 代码高度耦合，换个模型，换个应用就要重写大量代码，为了同时支持多个应用，需要把所有代码全部展开，不利于版本管理和代码复用。 调试极不方便，模型，编译，运行时每一层都有可能有出错的可能，但因为缺少明确的接口，基本上只能端到端调试。 因此，需要分层设计软件，解耦逻辑，复用代码。 从运行时的角度看，用户可能以多种形式使用： 直接使用，对硬件结构熟悉，通过运行时提供的接口简化操作。 接入框架，需要在算子内生成运行时所需的数据。 接入编译，接受编译层的通用中间表达（IR1），根据硬件结构转换成指令，屏蔽硬件细节。 调试使用，提供Python接口，方便运行调试。 通过合理的抽象，就可以满足使用需求，并保留足够的扩展性。 抽象 内存抽象 对驱动来说，其并不关心DMA下发的数据具体是什么类型，也不会关心内存和哪个具体模型对应。 对用户来说，其并不关心内存分配地址，也不关心硬件运行细节，最好作为函数接口调用。 因此，可以将内存抽象为三种类型： Blob。动态数据块，仅记录形状信息，需要在运行时传输，每次调用计算均会被改变。 Data。静态模型参数，编译时确定，声明周期和模型相等。 Instructions。计算指令，其中内存地址需要动态映射。 设备 设备（Device）抽象负责硬件设备初始化，设备分配，拥有设备相关执行逻辑。 设备抽象和硬件实现相关，通常和计算核对应，提供计算核申请与维护，内存申请与维护，指令解释（内存地址映射），数据重排，量化，精度转换等功能。 多模型 使用Executor机制，根据IR标识（比如哈希值或者校验和）区分不同模型。 对一个新IR，创建对应Executor以及State，并在Executor首次调用时调用Device的初始化接口，IR信息（比如计算核handler以及内存地址映射表等）保存在State中。之后调用Device的运行接口，考虑到多线程支持，State信息可能会被同时访问，仅设置为只读。 对单个模型的多线程调用，分配线程级数据结构保存当前调用的Blob分配信息，调用结束后销毁，避免加锁。 多后续标识相同的IR，通过cache表查到对应的Executor调用。 接口 也就是IR定义，主要包括： IR标示符 设备类型 动态数据表 静态数据表 计算指令表 其中还可以详细定义是否支持Dynamic Shape，是否需要完全内存映射等，量化精度，数据精度，数据对齐等。 优化运行时作为调用的关键路径之一，性能优化也相当重要，常用的优化手段包括： 资源池。 考虑到多线程调用时，每次调用的动态内存申请开销。在专用场景下，每次调用动态内存尺寸不变，可以使用资源池提前分配，并在每次调用时申请。资源池一般需要加锁，资源池大小可根据IR信息配置，也可开放接口供用户配置。 CPU Affinity 考虑到线程中断开销通常集中于CPU0，多线程使用时希望规划计算使用，也就是设置线程级CPU亲核性。开放亲核性配置供用户配置。 硬件Warm-UP 硬件pipeline填充也需要时间，首次启动时需要从DDR加载数据，针对特定应用，数据加载完成后不会反复swap，可以提供warm up接口，使用atomic变量，在首次运行时填充特定寄存器。 DMA合并 DMA的读写开销并不低，如果是IO数据块（处于计算首尾的需要和主存swap的动态数据块）较多，则可能拉低系统整体性能。驱动对内存连续性无法判断，可以在runtime内存申请时提前合并IO数据块，将DMA读写次数合并为1次，提高系统整体性能。 调试主要是C++编程的一些吐槽。 多线程安全 合理规划数据，优先选择不加锁。如果一定要加，根据场景加合适的锁，合理使用线程级数据/原子变量。 指针安全 野指针很危险。有时候系统通过原始指针对外需要暴露C API，但在进入C++部分后，需要合理使用unique_ptr，shared_ptr，std::move等限定和转移指针使用权。不然double free，空指针报错就很有可能出现，许多小项目在运行完之后抱一个指针错误，反正也查不出来是哪，就凑合着用了。 内存访问 数组下标越界引起的segment fault，在stack trace中并不能反应出来，很可能是程序欢快地运行很久之后突如其来，记得检查。查错时也可以上valgrind之类的内存统计工具，不过程序过大时，各种loss就可能让人眼花缭乱了。 内存泄露 内存泄露的排查也比较艰难，使用内存统计工具，排查各种指针使用情况之后，大概能解决绝大多数bug。但有些因为内存分配策略导致的内存使用量持续升高可能就和模型相关了（比如Tensorflow），需要具体分析。 [1] Intermediate representation","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 07 - 后端硬件驱动","slug":"ai/ml-system/hardware-backend/7-driver","date":"2021-01-22T09:49:45.000Z","updated":"2021-01-22T09:49:45.000Z","comments":true,"path":"ai/ml-system/hardware-backend/7-driver/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/7-driver/","excerpt":"驱动是对硬件接口的抽象和封装，设计时需要充分考虑分层和解耦。","text":"驱动是对硬件接口的抽象和封装，设计时需要充分考虑分层和解耦。 Linux系统将设备1抽象为文件，比如字符设备、块设备和网络接口，保持和文件一致的读写接口。 拓展卡类型的FPGA加速卡使用GPIO和PCIe相连，内置PCIe Controller和DMA提供统一寻址。DMA提供多组AXI总线（通常与DDR空间对应，DDR空间不足可高位截断）以及一组AXI-lite总线（通常和用户寄存器空间对应）。 FPGA硬件设备驱动的核心模块是DMA驱动，配合scatter-gather DMA，负责数据在主存和设备存储间的交换。 用户软件通过设备文件的读写接口发起大规模DMA读写，通过ioctl等系统调用定制接口，比如寄存器读写等。 FPGA芯片厂商通常提供DMA驱动，对FPGA开发者屏蔽了PCIe/DMA寄存器的操作细节。因此在驱动和硬件两侧看来，驱动设备文件的数据读写接口和AXI总线接口完全对应。 硬件加速器会基于FPGA厂商提供的驱动对应硬件功能做二次开发，一是向runtime屏蔽硬件操作的复杂性，复用操作逻辑，二是保护硬件，防止用户软件对硬件执行非法操作。 对于深度学习硬件后端，驱动的功能主要分为：内存管理，引擎管理，指令调度等。 内存管理驱动最简单的管理策略就是直接放开所有物理内存空间，用户可通过读写接口访问任意内存地址。 但这是个危险的操作，多个用户进程可能直接地址冲突，相互干扰，导致硬件逻辑错误。否则就要修改代码，对每个进程配置一个基地址。对于同构逻辑可能还行，如果是通用逻辑则无法保证均匀切割的地址空间能满足所有进程的需要。 因此，驱动应提供内存管理的功能，在用户进程发起写请求前，显式申请内存空间，并在进程退出后清理所有残留内存空间。 更为安全的做法是使用虚拟内存，将硬件的物理内存地址空间（有可能不连续）映射到连续的内存空间，相应的返回给用户的是虚拟内存地址。用户不能直接操作物理内存地址，同时对用户读写请求做内存地址所有权检查，禁止非法访问。 不过通常安全性和性能是相互限制的，在高频读写场景下，可能会影响系统整体吞吐。对于延迟敏感型应用可能有较大影响，应根据实际场景取舍。可以加入安全模式或低延迟模式的选项，满足不同场景的需要。 内存管理的另一个问题是，内存碎片。 如果采用直白的顺序分配方式，不仅在碎片数量上升的情况下，内存分配时间逐步上升，而且可能会在多轮地址全部分配后出现大量碎片，难以重新分配。 因此一个可以直接想到的方法是，预先将内存切割为大小均等的块，在内存分配时给出适合大小的块，使用bitmap记录内存使用情况。 指令内存和数据内存的大小通常相差很多，可以根据业务场景预设多种尺寸的内存块。这样会浪费一些内存空间，极端情况是场景所需内存恰好超出预设尺寸一些，必须要分配更大尺寸的空间，不过这样设计的好处是，内存管理逻辑相对简单。 如果要进一步优化内存使用率，可以使用均等切块，不连续内存访问的方式，由驱动控制数据的拆分，可能会将单次DMA读写拆分为多次。如此引入的额外性能开销，驱动复杂性和调试难度等，需要结合业务通用程度和团队产能平衡。 引擎管理密集型算子的计算核心对应硬件的计算引擎（计算核），同构结构下多个计算引擎的结构与性能完全一致，可能分别置于多个die上（有时候，为了压低单位算力成本需要高超的FPGA后端技巧）。异构结构可能分为大小核结构和完全异构结构。 使用大小核是为了充分满足设计上限（受限于给定可用资源或面积）。设计标准核的性能算力最佳，但占用资源较多。在设计上限内，不能完整容纳另一个标准核，但可以容纳阉割核（砍计算位宽，降频等）。但大小核功能基本一致，但是性能有差异。 完全异构核是指设计功能不同的计算核，比如拆分计算密集型和长尾通用型计算核，或者针对特定场景（比如推荐）设计特定计算核（比如Embedding）。 驱动设计时需要考虑上述各种情况，可以根据实际应用场景取舍。 一个硬件版本会对应一组硬件结构，可以通过配置文件的形式对应硬件寄存器版本信息。驱动在初始化设备时，创建对应的数据结构，不同类型的核对应不同的引擎类型。对外提供引擎分配，释放，启动，重置接口等。 指令调度对于完全同构引擎，驱动在分配时可以使用独占或共享策略。 独占策略声明引擎的排他性，通常是处于应用低延迟需要。由其是在没有引入指令交叉发射的机制下，共享同一个引擎的应用可能会被block较长时间，造成较高的延迟抖动。 共享策略是为了最大化吞吐，多个计算指令填充队列，屏蔽数据交换的overhead。 异构结构引擎的分配策略类似，需要考虑是否需要区分大小核。 实际上提前由用户进程分配物理引擎并不一定是最优方案，尤其是在用户进程没有全局信息的情况下。驱动可以负责指令的动态调度，使用虚拟引擎屏蔽物理引擎细节。 在分配引擎时，并不直接分配物理引擎，而是分配一个虚拟的引擎（持有类型信息），在实际启动引擎时，使用round-robin策略、按优先级或按负载调度指令。 在某些场景下，硬件物理引擎只有一份，但最大可以处理N条指令（队列深度）。驱动可以通过维护指令队列的方式和硬件结构对应，实时查看硬件的队列深度，但这样会引入轮询开销。 另一种做法是分配N个虚拟引擎对应一个物理引擎，并使用独占的方式，每个引擎阻塞执行，如此一来驱动就不必关心指令队列的实时状态了。总体上看，由软件调度的性能开销和抽象复杂度都较高。 另一种思路是将指令调度下沉至硬件，驱动单纯负责内存分配和数据交换，以及简单的引擎管理功能。 性能优化性能优化是必然要提到的，大体说来就是锁和合并。 多线程应用/多进程应用在驱动中的行为基本一致（除了内存共享），那么最应该注意的就是加锁问题了。 一个是mutex和spinlock，mutex会调用sleep，并切换context，spinlock则会反复尝试unlock直到成功。 另一个是精细加锁，不要在整个函数出入口加一把大锁，非常容易引起性能问题。 中断处理是需要消耗资源的，硬件过多的中断信号可能会丢失，或者造成CPU0占用过高，硬件可以提供中断合并的机制，降低实际中断次数。DMA的启动也有开销，因此需要考虑如何减少DMA读写次数，或者overlap启动开销。 [1] Intermediate representation","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 06 - 后端硬件软件栈","slug":"ai/ml-system/hardware-backend/6-software-stack-overview","date":"2021-01-21T08:47:50.000Z","updated":"2021-01-21T08:47:50.000Z","comments":true,"path":"ai/ml-system/hardware-backend/6-software-stack-overview/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/6-software-stack-overview/","excerpt":"没有软件支持的硬件就是一块砖，围绕硬件编写通用且易用的软件栈所需的资源以及重要性不亚于硬件开发。","text":"没有软件支持的硬件就是一块砖，围绕硬件编写通用且易用的软件栈所需的资源以及重要性不亚于硬件开发。 在硬件工程师主导的团队中，可能会忽视软件的作用以及开发工作量。 因为从硬件的角度来看，软硬件接口的抽象非常明确： 寄存器表 数据 指令 后端硬件通常暴露一组控制寄存器，上层软件通过驱动读写寄存器，完成初始化，复位，状态监测等操作。 后端硬件和主控CPU通过主存交换数据和指令，硬件指令解析器读取指令并发出相应的信号影响控制流和数据流。 然而软件工程需要在软硬件接口之上做多层封装，提供不同层次的接口抽象，满足不同需求的用户。 驱动（Driver）软件层最底层的封装，与操作系统耦合，提供最基本的硬件控制接口，基本和软硬件接口一致，比如读写寄存器表，读写存储器数据等。 但通常处于安全性考虑，驱动不会直接提供上面的功能，而是进一步封装，提供执行特定功能的接口。一个接口中，可能执行多个寄存器和存储器读写操作。 除此之外，驱动运行于操作系统层，天然具备跨进程的能力，因此在驱动也会负责跨进程的硬件状态管理，比如内存管理，指令调度等。 运行时（Runtime）用户使用驱动可以最大程度的控制硬件逻辑，但相应的所需要编写的代码量相当可观，因此，驱动层之上还会封装runtime。 runtime通常提供更高层次的抽象接口，进一步屏蔽硬件初始化，上下文（context）管理，设备管理，以及指令/数据的显示加载读取等操作。 总体上来看，runtime的性能相较驱动没有太多损失，但易用性大为提升，错误反馈和排查功能也更为友好。 因此，如果不是对硬件控制有特别高的需求，会使用runtime接口编程。 计算库（Libraries）编写硬件指令，由其是高效的硬件指令需要对硬件结构有深刻的理解，但是手工编写硬件指令对更高层的用户（比如算法工程师）来说是不够友好并且困难的。 因此后端硬件官方或第三方通常会提供特定领域计算库，提供常见计算pattern的接口，内置凝结了工程师智慧的手工优化代码，比如CuDNN，MKL-DNN等面向深度学习的计算库。 一般来说，硬件厂商提供的计算库性能高于第三方开发的计算库，毕竟对于硬件结构的理解更深入。但是第三方在应用上的创新速度远快于硬件厂商的响应速度，各家厂商对于应用的参与和主导程度也会影响计算库的迭代速度和易用性，比如NVIDIA和Intel。 计算框架（Framework）计算库提供了特定计算pattern的优化指令，尤其是对计算密集型算子，提高了硬件利用率。但对于一个特定应用来说，还需要足够灵活和高效的计算框架的支持。 对于深度学习应用，计算框架（比如Tensorflow和PyTorch等训练框架）提供了算子级别的抽象，使用Python定义DSL，让用户灵活构建计算图，并提供了反向梯度计算，方便训练。 服务框架用来将训练后的模型部署为在线服务，供业务应用调用，通常使用训练框架的计算核心，但是更注重面向网络接口的高可用，高并发等特性。 后端硬件通常只能支持主流框架全部算子的子集，需要接入框架提高可用性，否则，整个计算图都能在专用硬件上运行的概率不会太高。接入过程主要考虑调度，算子融合，以及常规的编译优化手段。 业务业务应用通常使用微服务架构，各个模块间相互独立。业务应用负责处理数据处理，格式转换，模型调用等任务。 业务应用从前端被动获取数据（比如用户的行为数据），或者从数据库/数据仓库中取数据，通过RPC1或HTTP调用机器学习模型在线服务，根据服务返回结果（比如物品推荐的打分）执行数据处理操作（比如筛选排列返回给前端的物品呈现顺序），优化某项业务指标（比如点击率或观看时长）。 机器学习服务有SaaS2化的趋势，对小微用户来说，招聘全职的算法工程师和平台工程师的风险偏高，且限于技术深度相对不可控。SaaS服务提供了更高层的使用接口，甚至是无代码操作方式，标准化托管用户的数据收集、处理、分析流程，根据用户的不同需要（比如推荐、风险控制、用户增长等）提供不同类型的服务。 影响基于数据分析的业务决策已经被实践证明在优化某些特定业务指标时效果明显，但数据分析逐步普及的趋势下，似乎陷入了电影院困境，大家被迫要用数据分析来决策。 数字化确实能带来效率提升，紧随其后的大数据计算利用机器学习在试图在微观层面上找到一些可以带来收益的价值，也就是所谓的挖掘数据中的金子。 智能化是在数据、算力双双获得爆发性增长后的趋势，但目前的智能化仍然只能看做大数据分析的进一步发展形态，依赖算力和数据规模暴力拟合。 因此而消耗的能源和人类时间，给环境造成的影响是否真正创造了等量的价值呢？ 正如模型训练步长过小容易陷入局部最优解，依赖于一个个AB Test做出的短期决策是否对长期价值有贡献呢？ [1] Remote procedure call [2] Software as a service","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 05 - 后端硬件后端流程优化","slug":"ai/ml-system/hardware-backend/5-fpga-backend","date":"2021-01-18T20:28:01.000Z","updated":"2021-01-18T20:28:01.000Z","comments":true,"path":"ai/ml-system/hardware-backend/5-fpga-backend/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/5-fpga-backend/","excerpt":"这里说的性能优化不是计算性能优化，比如脉动阵列的优化，而是如何使硬件设计在FPGA后端流程中时序尽量不违例。","text":"这里说的性能优化不是计算性能优化，比如脉动阵列的优化，而是如何使硬件设计在FPGA后端流程中时序尽量不违例。 FPGA的好处挺多的，可以芯片的价格也是美丽的。资源越多，价格越贵，而且还有不同资源配比，满足不同需要。一般来说，FPGA中的关键资源包括： DSP（乘加单元），代表了绝对算力，也是有一段短暂的时间FPGA可以和GPU叫板的主要支持因素。 BRAM（片上存储），代表了Cache大小，大到可以把模型整个都放上来，不过实际上只需要放下提前prefetch的数据即可，通过pipeline屏蔽DDR读取latency。 FF（锁存器），代表了寄存器总量，一般不太会不够用吧。 布线资源，影响通用逻辑复杂程度。 LUT（查找表），影响通用逻辑复杂程度。 那个时候，大家的工艺节点还差不多，Xilinx猛怼DSP，单个封装内3D堆几个die，加上FPGA工程师使用的倍频技巧，实际计算效率还能和GPU比一比。 后来，NVIDIA财大气粗，逐步垄断市场，算力每代大跃进。Xilinx起初还想以ACAP1——同时集成ARM、FPGA、CGRA的巨兽——反超。 但无奈ACAP还是难产，大部分时间停留在励志吃下5G、自动驾驶、人工智能等多个领域的PPT中。即使真的大规模量产，成本也是可以想象的没有竞争力。最终，做FPGA加速器的，还是老老实实回去做芯片了。 FPGA后端指的是Synthesize和Implementation，Implementation包括Remapping，Placing、Routing等。 相比芯片要傻瓜的多。通过XDC约束布局布线，更换策略，画pblock等方法降低违例TNS（Total Negative Slack）。 通过阅读STA报告，我们基本能定位有问题（比如fan out过高，WNS明显过小，拥塞等级过高）的地方，反过来修改设计，比如插入寄存器，对跨时钟的逻辑set_false_path，或者手动复制寄存器等等。 DSP一般被用来搭建计算密集型算子，元素积或者自然指数通常用LUT搭乘法器，根据频率调整合适的pipeline级数，前后插入寄存器提高时序。 控制跨die信号总量，跨die频率尽可能的低，前后加寄存器。 多阅读官方文档，熟悉底层器件特性，明白自己写的代码映射到什么样的硬件结构上，参考示例工程。 Garbage in, grabage out。 [1] Versal","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 12 - 推理优化总览","slug":"ai/ml-system/inference-optimization/12-overview","date":"2021-01-15T17:28:01.000Z","updated":"2021-03-04T07:22:23.195Z","comments":true,"path":"ai/ml-system/inference-optimization/12-overview/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/inference-optimization/12-overview/","excerpt":"模型推理服务是在业务关键路径上的，其性能足以收到高度重视。如何优化训练后的模型，提高性能，降低资源占用，维持推理精度，优化方法的通用化与自动化，这些是模型推理优化的核心目标。","text":"模型推理服务是在业务关键路径上的，其性能足以收到高度重视。如何优化训练后的模型，提高性能，降低资源占用，维持推理精度，优化方法的通用化与自动化，这些是模型推理优化的核心目标。 说起性能优化，软件的性能优化应该是必备功课，通过profiling定位bottleneck，再针对性优化，可以说具备一定程度通用的方法论，但实际业务上也要随机应变。 模型性能的优化也属于软件性能优化，特点是模型的运行调度框架已经确定，并且由社区平衡通用性和性能做了不少的优化。 模型性能的问题主要在低效的计算密集型算子，不合理的子图结构，复杂的动态运行环境，冗余的模型结构和数据宽度。 针对这些问题，模型推理优化方法主要分为： 计算图结构优化主要是清理冗余节点，一般的训练节点，框架在开始运行前会根据输入和输出裁剪无关节点，通常对性能影响不大。 但部分嵌入计算图的节点就很难被去除，Tensorflow引入的控制结构就是是图结构优化点之一，比如训练时需要的随机dropout结构等，用户可能在构建模型时把标记是否是训练的变量加入到计算图，在清理时就需要将对应placeholder转变为常量，提取子图。 算子融合主要是融合子图，比如对于Feature Embedding这类常用的特征预处理API，通常为了灵活性，生成的子图比较复杂，算子数量极多，框架调度算子的开销远远超过算子实际执行的开销。因此可以匹配对应子图，使用一个算子代替，大大提高执行效率。高频模型中Tensorflow引入的循环控制结构也可以被优化，比如LSTM结构。 通常需要匹配识别的子图较复杂，依赖人工发现模式并编写匹配逻辑，基本纯手工，开发验证工作量大，由于引入了控制节点和Dynamic Shape，目前编译优化方法也没有很好的解法。因此，通常只针对成熟模型做此类定制优化，比如Transformer结构，Embedding结构等。 硬件计算库底层硬件开发者出于安全或其他原因考虑，不会开放底层实现细节，也不会开放精细控制接口，但为了迎合市场需求，会推出计算库。 计算库内提供了粗粒度的算子接口，内置了不少汇编层级的优化经验，实现方法至于是if else还是JIT codegen就不得而知了。 计算库内的核心是如何高效利用硬件执行矩阵并行计算，通常基于经验，比如考虑Cache大小，内存带宽，数据位宽等，利用一个“经验性”公式计算出合理的矩阵分块配置，比较重要也比较让人感到tricky的是公式中不时出现的magic number，很有可能是结合profiling得出的经验。 常见的计算库比如CUDA和MKL-DNN，在单个算子优化的基础上，硬件厂家还会提供子图层面的计算优化库，比如TensorRT等。 编译优化编译优化包含一大类方法，也借鉴了上述方法的思路。主要目标是自动化地、通用化地实现图结构优化、算子融合、以及矩阵分块。 和编译器的优化思路类似，不同框架的计算图经过分图，将可编译的子图会被统一转化为编译优化框架的中间表示（IR），再通过一系列的通用的编译优化pass得到优化后的IR，随后codegen生成执行代码。只要能被划分进编译子图，就能通用化地支持图结构优化和算子融合。 最精华的地方在于codegen，主要思路有： 写入人工调优的经验，生成高效的底层执行代码。一般是社区早期这么做，比如Tensorflow的XLA。 借用硬件计算库生成高效代码。硬件厂商这头大象终于反应过来了，毕竟他们对硬件更了解，不用白不用。 把硬件当作黑盒，构建细粒度矩阵分块方法，通过profiling不断调整分块参数，直到收敛到最优解。比较暴力，分块方法的可调参数越多，Tuning（参数搜索）的时间可能越长，不过也有算法加速收敛，比如TVM。 解析法，比如Ployhedral，把矩阵分块过程描述为一般的空间变换方法，而性能优化问题就转化成了搜索最优整数解的过程。愿景很好，但目标函数本身又很难和实际硬件完全对应，搜索的时间也不小。 编译优化方法在学术届内百花齐放，本质上还是新瓶装旧酒，关键还是在于如何把新问题转化为通用的老问题，并解决新的问题定义边界带来的新的约束带来的corner case。 对于主流模型，硬件厂商的人力可以做到全图支持，定制优化的性能让编译优化难以超越，编译优化更多地是在通用化的corner case场景下发挥作用，与手工方法互补。 同时现有的编译优化方法本身也会遇到比如Dynamic Shape的问题，严重影响分图和codegen，亟待解决。好在如MLIR等编译基础设施工作正在不断发展，解决多层IR信息联合优化，复用优化pass，解决共有的问题。 模型压缩除了编译优化方法之外，模型压缩也被用来从算法层面去除降低数据位宽，冗余图结构，大幅并行化模型结构。 量化 模型推理的性能问题有很多是memory bound（受限于内存带宽），尤其是在高度密集计算（大batch）的场景下，同时推理相对于训练不需要很高的数据精度，可以使用量化方法缩减数据位宽，直接降低所需内存带宽，memory bound问题也迎刃而解。同时原来用于计算32位数据（FP32）的计算单元可以配置为计算2个16位数据（FP16）和4个8位数据（INT8），计算性能也直接翻番。 剪枝 深度学习模型并不是要求精确计算，而是计算概率分布，因此在实践中，将模型中权重较小的连接直接置为0或去除，对模型的整体精度影响不大，反而有效降低了总体计算量。 蒸馏 出于直观，可能使用串行结构（数据依赖）模型，导致计算效率不高，或者使用超大模型，模型参数非常多。但模型训练本质上是拟合数据的目标概率分布，那么就可以使用另一个并行模型“学习”串行模型的概率分布，或者使用一个小模型“学习”大模型的概率分布。只要模型精度控制在预期范围内，那么所需的计算量就能大幅下降。 推理服务性能优化这部分和模型关系不大，但也在关键路径上，因此放在这里。服务的性能优化，主要是优化数据链路，比如数据传输协议，Serving框架线程性能，网关性能等。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"推理优化","slug":"推理优化","permalink":"https://blog.kleon.space/tags/推理优化/"}]},{"title":"机器学习系统 04 - 后端硬件验证","slug":"ai/ml-system/hardware-backend/4-verification","date":"2021-01-15T16:12:01.000Z","updated":"2021-01-15T16:12:01.000Z","comments":true,"path":"ai/ml-system/hardware-backend/4-verification/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/4-verification/","excerpt":"仿真不是真，只不过是现实世界的一个有限切面。但仿真又是在进入现实世界前的必经试炼。","text":"仿真不是真，只不过是现实世界的一个有限切面。但仿真又是在进入现实世界前的必经试炼。 仿真（Emulation）用来从底层机制层面模拟硬件运行性能，测试功能正确性，评估性能是否达到设计标准。 层级项目启动阶段重心放在硬件功能开发上，而验证流程可能处于刀耕火种阶段： C++写Test Case，dump输入数据文件和输出结果文件，要求转化成Verilog-friendly 16进制字符串 手写Verilog读取文件，围绕仿真激励逻辑，dump波形文件和输出结果文件 diff比对Test Case和Verilog的输出结果文件，匹配不上需要看波形定位问题 实际生产中需要完善的仿真测试来保证达到设计目标，尤其是芯片，毕竟比较贵。如果流片回来变砖就很尴尬了，赶在流片前，尽力而为。 和软件测试类似，硬件验证也分为不同层级。根据粒度可以将仿真分为： 单元仿真 测试基础功能模块是否达到设计目标，通常设计一些测试用例（Testbench）来验证，基本作用与软件单元测试（Unit Test）类似。 软件测试的基础单元是函数（Function），硬件测试的基础单元是模块（Module），需要选择合适的测试粒度大小。 TDD1（Test Driven Development）适合拆解开发工作量，也可以用于硬件开发。 由于HDL及其周边工具的迭代速度缓慢，语言编程接口与仿真调试工具的易用性与软件相比，并没有显著性提升。因此在以开发速度远超ASIC闻名的FPGA开发流程中，单元仿真可以是不存在的。 至少在一个项目进度很紧的早期团队中是不存在的。 系统仿真 这里基本实现了较独立的完整功能。为了提高仿真速度，一般去除比较独立的寄存器逻辑，仿真非常耗时的存储器（如DDR仿真模型）逻辑等。 即便去除了一些耗时的逻辑，硬件仿真的耗时相比软件高出不少，并且还没有测试Cache机制，每运行一次可能需要数个小时，导出的波形文件也非常大。 除了一些用来跑通控制流和数据流的Case之外，一般也会采用遍历（case较少）或者随机测试，最终看代码覆盖率。如果要做比较完善的测试，靠刀耕火种的流程肯定是不够的，自动化（Automation）是必须的。 后仿真 后仿真，是指可以在每个后端流程（综合，布局布线）后抽取HDL仿真。用于检查流程是否出错，以及时序验证。前端也可以手动加入一个delay（不会被综合，但可以指导仿真），模拟信号固定延迟，波形上好看一些。 FPGA几乎从不后仿，非常费时，不如可以直接上板，通常一些只有上板才出现的问题也仿不出来。 上板调试 通常使用FPGA开发板，因此称为上板调试。在芯片验证流程中，FPGA被用来做原型验证，一般降频运行，测试功能时序正确性。 容易遇到的问题是，测试模型需要用软件重写，导致引入额外bug。 如果真的遇到了和预期不符的地方，调试起来就比较费力。需要使用FPGA IDE的信号抓取功能添加信号线作为debug逻辑，再重新跑后端流程。 当FPGA项目比较大时，一次可以抓取的信号数量有限，需要同时编很多版本。在FPGA资源已经比较吃紧的情况下，拥塞程度较高，添加过多信号线，会让时序优化更有压力。 实际测试下来发现，很多bug都是因为Test Case不够全。因此要尽可能在之前的仿真中测出bug啊！ 如果遇到了抓信号也不好复现的bug： 有可能是跨时钟问题，这个在仿真中不太好复现。 有极小可能（真的碰到过）是定制板卡的PCB版设计有问题。 几乎没有可能是因为宇宙射线引起的单粒子反转效应。😃 应用测试框架可以使用System Verilog的UVM2（常用的芯片工业的验证框架）或者基于Python的Cocotb3。 深度学习的FPGA后端的核心功能是offload计算密集型算子，并且与标准框架（Tensorflow，PyTorch）实现比对数据，对验证的覆盖性要求不高。可以使用基于Python的Cocotb，使用Python编写测试逻辑的好处是有大量可用包，写数据处理逻辑比较方便。 Cocotb只提供了核心组件和System Verilog的VPI4连接，需要编写硬件激励的Driver，把Python数据转换成硬件信号，比如AXI/AXI-lite Driver；编写信号采样Probe，把硬件信号转换成Python数据。 向上封装类DMA的驱动接口，包括DMA数据读写以及寄存器读写。这样在这个层面可以做到和驱动接口统一，便于在上板测试时保持上层激励逻辑不变。 再向上封装硬件相关的运行时（Runtime）模型，直接配置硬件指令，运行时模型负责模拟内存分配，数据和指令读写等功能。使用者只需要指令遍历指令的集合验证即可。 由于Verilog仿真速度很慢，在指令遍历选项过多的情况下会非常耗时，一般选择直接上板遍历，并记录对应失败配置，有针对性地对失败的case仿真调试，查看波形。 收获的一个经验是： 测试越早写，收获越大，可以有效减少系统调试时一些花了很久定位的“显而易见”的bug。但目前还不知道怎么能提高写测试的积极性，降低写测试的overhead。 绝大多数代码只会被用一次。 [1] Test-driven development [2] Universal Verification Methodology [3] cocotb/cocotb [4] Verilog Procedural Interface","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 03 - 后端硬件描述语言","slug":"ai/ml-system/hardware-backend/3-hardware-description-language","date":"2021-01-14T16:52:01.000Z","updated":"2021-01-14T16:52:01.000Z","comments":true,"path":"ai/ml-system/hardware-backend/3-hardware-description-language/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/3-hardware-description-language/","excerpt":"语言只是手段，不是目的。自然语言如此，软件编程语言如此，硬件描述语言也如此。","text":"语言只是手段，不是目的。自然语言如此，软件编程语言如此，硬件描述语言也如此。 软件编程语言定义了一系列面向人类可读性的字符语法。人类使用软件编程语言编写描述机器如何处理信息的步骤的文本，简称为软件代码。使用编译软件使用指定指令集翻译成机器具体处理信息的指令，简称为程序。🤓 类似的，在硬件设计中，硬件的结构决定了功能，因此也需要借助一种文本描述硬件结构，得名硬件（结构）描述语言。 软件代码可以直接编译成程序运行，也可以直接和测试框架一起编译运行，测试功能正确性。 而硬件描述语言（HDL，Hardware Description Language）——其中对应硬件结构（可综合）的合法语法——只是对硬件的结构描述，需要使用仿真器（Emulator）构造硬件结构。 通常高效的仿真器会将硬件代码等价映射成软件代码（比如C++）编译后运行，提高仿真效率。 就可综合语法部分看，HDL乏像软件语言中的抽象模型，以用于代码复用。但是HDL中的寄存器、线以及模型足以描述任何复杂的硬件。可以利用软件工具管理生成代码，减少一部分coding的工作量。 比如使用脚本（Perl或者Python）管理模块，辅助连线，或者借用软件的OOP（Object Oriented Programming)模型，套壳做DSL（Domain Specific Language）（比如Chisel套在Scala里），借助套壳语言的语法和抽象简化硬件编码工作。 除了可综合语法，HDL中还引入了测试语法用来提供仿真激励。为了改进HDL的测试方面的短板，更多语法和抽象被引入进来，以至于可以被称为一门新的语言（比如System Verilog1)。 有趣的是，System Verilog可以被看作天然的并行语言，引入不少并发机制，比如channel，GoLang的并发机制里也有channel。 硬件开发的主要工作量并不在前端编码（主要包括编写Verilog并做功能仿真），占大头的是仿真测试和优化。在一些非自主开发IP的流程中，前端的工作基本就是把买来的IP（比如ARM）集成连线，写一些胶水逻辑。在一些自主IP定制的流程中，比如RISC-V，主要的功能还是在拓展指令集，然后接着大量测试和优化。 我曾经试图发明一门新的语言，使用现代语言的语法来改进Verilog。写好了词法Parser，拿到了AST，做了一些类型推断以及代码优化工作，可以生成Verilog。然而如果要用于生产，开发工作量是不小的。最终只是作为一个toy项目，帮我熟悉了一些编译器的知识。 现在回过头来看，如果考虑写一门HDL的话，需要考虑几点： 是否可以适配已有工业流程？通常是生成Verilog和其他EDA流程对接。 是否可以做到和Verilog可综合语法完全等价？需要形式化验证以及实际项目支持（比如Chisel傍上了RISC-V）。 是否支持其他blackbox（加密IP网表，VHDL，Verilog，SystemVerilog仿真模型）联合仿真？ 是否能在保证仿真结果精度准确的情况下，提高仿真效率？只能粗粒度功能仿真就很鸡肋。 是否可以引入稳定的自动优化？怀疑是否又必要，后期优化的时候基本会扣每个寄存器，后端流程也有大量优化。 是否易学？过多的引入严格的抽象概念会大幅提高语言学习难度（用软件语言的角度类比，学go，可能3天上手写工程，学rust，学一个月可能还经常卡在工程编译上），HDL依赖惯性可能比软件语言要大的多。 软件语言的迭代，引入了新的编程模型，新的内存管理机制，新的并行模型等等，实实在在地带来了极大的收益，大幅提高了并发性能，大幅减少了代码量，大幅降低了运行时排错的成本等等。 最终决定新语言是否会被大规模使用的核心还是：开发学习使用新语言引入的成本是否带来了足够的生产力收益。 HDL的核心功能还是描述硬件结构。 如果高层语言可以直接生成Verilog，要么是面向FPGA的HLS2（High Level Synthesis），面向软件工程师的硬件加速，根本不关心生成的代码，生成的代码也不可读。 要么是面向芯片的设计流程，语言本身最好能和硬件资源一一对应，所见即所得，否则就要生成可读的代码，让工程师检查。不过工程师既然都要人肉检查生成的代码了，再套一层壳就比较鸡肋了。 HDL已经深深嵌入到整个芯片设计流程中，前端和后端都能统一，EDA工具掌管着优化，VCS和Verdi已经是测试的标准工具。相比软件语言，HDL是一个非常细分且小众的DSL。 也许，之后会有新的HDL成为事实的工业界标准，但应该会是一个漫长的过程。 [1] System Verilog [2] High-level synthesis","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 02 - 后端硬件指令集","slug":"ai/ml-system/hardware-backend/2-instruction-set","date":"2021-01-13T15:14:01.000Z","updated":"2021-01-13T15:14:01.000Z","comments":true,"path":"ai/ml-system/hardware-backend/2-instruction-set/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/2-instruction-set/","excerpt":"定制计算硬件的一个重要任务是确定指令集1。对于深度学习模型，则要首先确定模型集，继而确定算子集，最后统计出指令集。","text":"定制计算硬件的一个重要任务是确定指令集1。对于深度学习模型，则要首先确定模型集，继而确定算子集，最后统计出指令集。 划分总体上看，指令按计算密度分为计算密集型算子指令与长尾算子指令。 计算密集型算子指令通常就是指矩阵乘加计算，其他密集型算子（比如深度学习定义的卷积运算）可以通过变换转化为矩阵乘加。 因此矩阵乘加是定制硬件的核心，也是模型优化的核心。 换句话说，定制硬件（包括GPGPU）的主要设计工作在于如何在面积功耗约束下放下更多的矩阵乘加单元，同时提供匹配算力的存储访问带宽。 模型优化的主要内容就是在于如何合理排布硬件的矩阵计算指令和存储访问指令，使矩阵计算模型的实际利用率（有效计算时间占总运行时间）尽可能接近硬件的理论算力上限。 长尾算子指令则是针对出现在模型中但计算量不太大的算子设计的指令，但总体而言模型的复杂程度和改进速度都比硬件开发速度快，不可能浪费芯片面积支持所有的长尾算子，因此设计目标按优先级排序是： 如果长尾算子不在模型首尾，而出现在两个密集算子之间，并且CPU计算开销（通常指耗时）相比交换给CPU计算（内存交换以及中断/轮询）的开销小，就应该尽量使用硬件计算。 归纳必须支持的算子，抽象出更通用的计算逻辑，尽量最小化占用的芯片面积。适配计算密集型指令算力和长尾指令算力，在多种目标场景中统计，长尾指令尽量不成为性能瓶颈。 评估大型模型（不能完全放在芯片内存中，需要和CPU主存交换）在目标承载系统中的算力适配程度，如CPU算力是否能支持内存拷贝开销，中断开销，硬件不支持的长尾算子的计算开销，系统其他服务开销。 粒度指令是硬件向软件开放的“操作接口”，指令的粒度则决定了软件对硬件控制深度。 定制硬件的早期，尤其是针对某个特定模型的特定设计，软件和硬件都是及其专用的。硬件基本包揽了模型端到端的所有计算，甚至连模型结构也以一种“硬编码”的方式写在了驱动中。 硬件的整体架构只有两部分：一部分是寄存器，给软件配置来启动计算步骤，另一部分是一条巨大的流水线，归纳了整个模型最基本的计算结构，比如对于ResNet50，就是卷积-元素积-仿射变换-激活ReLU-池化。 软件运行时（Runtime）部分几乎就是很薄的一层，甚至可以大部分写在驱动中。驱动主要负责启动DMA读写数据以及配置查询寄存器。运行时只需要从某个地址读入输入数据，配置一个寄存器，接收到中断信号后读回数据写到另一个地址。 如果把配置寄存器也看作指令的话，那这就是最粗粒度的指令，软件没有控制的余地，指令可以写作RESNET50 🙂 。 逐渐地，需要支持更多相似模型，不同层数，不同长尾算子，则需要按层来配置卷积大小，长尾算子是否计算，有无pypass，内存数据如何存放等。硬件的流水线也配置了更多寄存器开关支持不同的计算路径。 这就是单层粒度的指令，软件可以通过一大组寄存器灵活配置每层的属性，也有了多模型interleaving（同时交叉）计算的空间。此时的指令可以写作LOAD-CONV-ELTMUL-AFF-RELU-POOLING-STORE 🙂 。类似粒度的硬件可以参考NVDLA2。 软件的功能逐渐增多，在运行时（Runtime）上增加编译器部分，将相近的算子变换为硬件支持的指令。硬件后端支持的模型更多了，但实际计算效率可能有所下降。这也是一个常识————通用性会牺牲专用性的性能，比如Windows和MacOS，CPU和GPU。但群体智慧的迭代可以逐步弥补通用性带来的损失，比如RISC与CISC，Android与iOS。 随着硬件的通用化，几乎不可避免地会向主流已验证设计靠拢，卷积计算将不再特例化，不断增加的变种支持会使硬件复杂度越来越来高。最终不得不回归本源————矩阵乘加，原本的超长流水线设计也会自然分化成两部分————密集计算单元与长尾计算单元指令。 密集计算单元由于去除了特例化设计，密度可以进一步提高；长尾计算单元将进一步整合，提高通用性。此时的指令进一步细化为LOAD，MATMUL，MULT，ADD，EXP，STORE等。指令操作的精细程度也进一步提高，类似SIMD操作。 多个计算单元和局部缓存构成一个计算引擎或者计算核，多个核可以由软件或者硬件直接调度（指令发射顺序），编译器的复杂度进一步上升，可以在更细粒度调整指令顺序，考虑缓存以及内存带宽匹配（Memory-Bound）。到了这个粒度的指令离GPGPU也不远了。 指令集基本划定了硬件可能的应用边界，不过实际硬件设计实现过程中还有其他要考虑的因素，取决于芯片后端与FPGA后端需求，略。 [1] 指令集架构 [2] NVIDIA Deep Learning Accelerator","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"},{"name":"FPGA","slug":"FPGA","permalink":"https://blog.kleon.space/tags/FPGA/"}]},{"title":"机器学习系统 01 - 后端硬件初探","slug":"ai/ml-system/hardware-backend/1-overview","date":"2021-01-12T12:58:36.000Z","updated":"2021-01-12T12:58:36.000Z","comments":true,"path":"ai/ml-system/hardware-backend/1-overview/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/hardware-backend/1-overview/","excerpt":"机器学习系统通常使用定制硬件后端执行计算密集型算子。训练模型中包含大量训练专用的反向梯度算子，相比之下推理模型结构更简单，算子集更小。因此定制硬件通常从推理模型入手。","text":"机器学习系统通常使用定制硬件后端执行计算密集型算子。训练模型中包含大量训练专用的反向梯度算子，相比之下推理模型结构更简单，算子集更小。因此定制硬件通常从推理模型入手。 分类如果采用异构计算系统的定义，硬件后端可以大致分为： CPU 对应最通用的计算设备，可以执行所有计算，灵活性最高，计算性能通常不够高。不过由于CPU的广泛通用性以及较低的成本，使用CPU做推理计算也很常见。 因此针对不同指令集的CPU也发展出了不同的优化方案，比如Intel加入了AVX512指令支持超宽SIMD指令，但直接加入编译选项对性能提升不明显，需要使用集合了Intel汇编工程师的智慧的MKL-DNN库，针对不同算子精心设计精细到cache-line的操作；Intel随后几代CPU又加入了VNNI指令（INT8支持，实际使用时也有限制）和BF16指令（详细见推理优化）。 GPGPU1 对应计算密集型通用计算设备，可以执行有限的通用计算，取决于设备指令集。市场现状是，NVIDIA几乎垄断服务器级GPGPU供给，成本较高。CUDA软件栈闭源，优化通常只能基于NVIDIA提供的接口。不过也有一些奇技淫巧，通过接口劫持绕过CUDA软件的部分限制，需要一定的逆向工程的能力。 NVIDIA在模型量化（使用低位宽存储模型参数）上也很激进，INT8，INT4，FP16领先加入硬件。随之也出现过一阵超低精度paper热，使用2bit/1bit存储。如今，量化方案基本成熟，使用FP32训练，使用FP32直接推理，使用FP16/BF16推理精度略有，使用INT8则可能需要重新训练以恢复精度（详细见模型压缩）。 NPU 对应最专用的计算设备，通常支持的指令集最小，比如仅支持矩阵乘法，卷积，以及较常用的高频算子，执行效率最高。 几年前（现在是2020年）FPGA2凭借其相比ASIC3（专用定制芯片）的灵活可编程能力，一度出圈，成为深度学习硬件加速器的宠儿。不过，如今泡沫退去，一大批ASIC创业公司纷纷倒闭，FPGA自然也退回其最基础的应用场景（原型验证，低频可编程硬件）。 与此同时，NPU作为芯片的必备模块，已经与ARM整合进入手机CPU。一些大公司财大气粗，内部也在使用自研的特定场景芯片。 专用硬件突出“专用”二字，这意味着可以将应用场景无限缩窄。可以针对一类模型，甚至一种模型，甚至几种算子，针对性地使用大量专用优化。这样固然可能取得不错的性能，但是硬件开发本身相比软件来说慢得多。芯片的开发周期通常以年计，FPGA开发可能以月计，软件开发通常以周计。相较之下，必须要权衡开发时间、性能、成本以及通用性。 FPGA在芯片原型阶段的优势明显，如果大规模量产则成本与功耗都远高于ASIC。 另一方面，随着GPGPU的不断发展，先进工艺节点夹持，性能大幅提高，FPGA早期的低成本性能比的优势不复存在。在泡沫时期可能存在诸多在多项指标中平衡的特定场景硬件，但最终还是互相合并，收敛到几种硬件设计方案。 不过FPGA作为不错的早期原型平台还是有价值的，前端硬件的设计思路也和芯片设计相通，可以借鉴。 [1] 图形处理器通用计算 [2] Field-programmable gate array [3] Application-specific integrated circuit","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"}]},{"title":"机器学习系统 00 - 总览","slug":"ai/ml-system/0-overview","date":"2021-01-11T09:08:04.000Z","updated":"2021-01-11T09:08:04.000Z","comments":true,"path":"ai/ml-system/0-overview/","link":"","permalink":"https://blog.kleon.space/ai/ml-system/0-overview/","excerpt":"机器学习1模型对应一类用于从数据中提取模式与特征的人工智能子领域算法。机器学习系统用于更便捷地、更高效地、更自动化地构建机器学习模型，与数据分析系统整合。","text":"机器学习1模型对应一类用于从数据中提取模式与特征的人工智能子领域算法。机器学习系统用于更便捷地、更高效地、更自动化地构建机器学习模型，与数据分析系统整合。 机器学习是数据分析的延展，得益于算法改进（Algorithm)，数据快速增加（Big Data），算力提升（Computing Power），发掘了依赖数据统计理论的业务价值，深度学习模型被大规模应用。 随之而来的，是对快捷便利地构建部署模型的需求，从实验室的原型验证逐步过渡到成熟的大规模机器学习系统。 架构 机器学习系统根据构建阶段可以分为： 数据管理 机器学习的核心是数据，这些数据通常来自业务系统的数据仓库，通过ETL2系统清洗提取后组织成统一的结构化数据。 数据管理子系统用来管理用户导入的结构化数据，称为数据集（Dataset）。通常数据管理子系统提供数据标注功能，由数据工程师或者众包人员标注。 模型开发 模型的开发与调试需要算法工程师参与，通常使用Python脚本或Jupyter Notebook。常用框架有Tensorflow3 / PyTorch4。开发人员更注重IDE的便捷性，而小微开发者对算力价格更敏感。 模型训练 模型调试完成后需要大规模训练，通常会调整超参同时启动多个训练任务，这需要大量的算力，通常需要使用分布式训练系统。常用框架有PS（Parameter Server） / All Reduce / Horovod。同时，训练任务的实时参数需要及时(比如通过Tensorboard)反馈给算法工程师，以便调整训练参数。 模型部署 模型训练完成后，挑选精度/计算量符合业务需求的模型优化部署。模型部署系统需要支持多种标准模型，并且提供后端服务直接调用的接口，注重高吞吐、低延时。常用框架有Tensorflow Serving / TorchServe / Neuropod。在模型部署前，可以有针对地优化模型，以达到提高计算效率，降低推理延迟，节约计算成本等目的。 模型开发人员在首次开发新模型的过程中，需要手动完成模型的训练和部署。之后可以构建自动化训练部署的流水线（蓝色循环），定期或按特定条件触发。 我的经历是从后端硬件加速器入手，逐步扩展到后端硬件软件栈，模型推理优化，到机器学习系统的架构与产品。从下一节开始，我会逐一展开讨论。 [1] 机器学习 [2] ETL [3] Tensorflow [4] PyTorch","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.kleon.space/tags/机器学习/"}]},{"title":"如何掌握复杂事物","slug":"think/complexity","date":"2020-04-18T18:23:19.000Z","updated":"2020-04-18T18:23:19.000Z","comments":true,"path":"think/complexity/","link":"","permalink":"https://blog.kleon.space/think/complexity/","excerpt":"理论往往是优雅简洁的，但是现实一定是复杂的。为了解决现实问题的理论通常会愈发复杂，解决方案可能会最终超越复杂，变成混乱。","text":"理论往往是优雅简洁的，但是现实一定是复杂的。为了解决现实问题的理论通常会愈发复杂，解决方案可能会最终超越复杂，变成混乱。 定性究竟该怎么定义复杂？复杂在不同人看来可能很不相同。 正在背诵乘法口诀的小学生可能觉得复杂，正在理解理论物理公式的大学生也可能觉得复杂；正在计算购物节满减凑单优惠的剁手党可能觉得复杂，正在试图理解经手多人外包的代码的程序员可能觉得复杂。大学生看小学生的乘法口诀觉得小菜一碟，而程序员可能不一定能理解满减规则。由此看来，复杂性是由概念数量决定的，需要同时考虑的概念数量越多，就会感觉到越复杂。并且，复杂性与一个人的知识积累以及抽象能力相关，小学生背诵乘法表需要记忆。 我们面对如毛线团般的事物无从下手，而经验丰富的专家则能抽丝剥茧，重新梳理编织得井井有条。那么我们与专家的区别在哪里呢？答案就在知识储备上。 知识储备的一种理解是，如果把我们的思维比作工具箱，把碰到的问题比作奇形怪状的钉子，那么我们的工具箱中有多少种可选的锤子可以帮助我们敲钉子，就代表我们的知识储备的丰富程度。同样是看到一粒凸起的钉子，普通人（未经过专业训练的人）只会觉得无从下手，光是想象自己赤手空拳跟钉子软碰硬时，就觉得疼得要命。而专家可能会掏出他惯用的锤子，敲敲打打，即使不能把钉子敲好，至少手中有锤，心里不慌。反映到现实中来，普通人如果看到一件事的发生，可能不知从何理解，甚至陷入对复杂性的恐慌之中，大脑一片空白。不过普通人绝不止这一种反应，比如还可能回忆起自己一知半解、道听途说来的概念，强行编造个因果关系。如果我们能把现实简化，抽象出概念，建立模型，则能帮我们更好的理解事情，从而找到可能的解决方法。 值得注意的是，工具箱思维也只是对知识储备的一种片面解释，如果眼里尽是钉子，那不得跟肉中刺一样疼吗？。 模型神经生物学揭示了大脑复杂的物理结构，我们目前很难自底向上地完全解释大脑的运作方式，不过我们在不同层次上建立了几种解释模型。一种是基于信号测量的脑区划分方法（需要准确名称），这种方法通过实验测量大脑不同区域的活跃程度与行为之间的关系。另一种是更抽象的概念模型，用行为来验证模型的适用范围。 认知心理学中就有一个“工作记忆”的模型。模型由长期记忆和短期记忆构成，工作记忆属于短期记忆的一种，相当于思考的工作台，而长期记忆相当于记忆的仓库，用于存储暂时用不到，但可能日后还有用的记忆。每个人的工作记忆大小不同，每个概念占用一定的空间，如果概念数量太多以至于超过了工作台的最大容限，思考将变得困难。我们可以把外界中的概念与工作记忆“交换”，比如用纸或者白板写下来，但因为高速的“内存”和低速的“外存”直接需要不断交换信息，思考的速度将下降。 另一个模型是关于快慢系统的：我们的大脑可以简化成两种思考系统，一种是系统1，通常是迅速的，潜意识的，反射式的，不由自主的；另一种是系统2，通常是缓慢的，费力的，需要意识参与的，可以控制的。涉及的所谓“肌肉记忆”的技能，以及需要快速反应的“机械知识”都可以通过反复练习固化下来，直接经由系统1做出反应。而学习、分析、推理、想象等行为则需要系统2参与，这时影响思考效率的就包括同时处理的概念数量了。 我们感受到的复杂通常是系统2的产物，我们不会觉得辨认普通地辨认猫或者其他常见动物是件复杂的事。这通常在一瞬间发生，当你看到一只典型的“猫”样的物体时，猫这个概念几乎不可阻挡的进入脑中，这也是系统1的特性，它是基于神经系统的。至于接下来的处理步骤就取决于你当时的状态，如果你的注意力集中在别的事情上，那么这个概念也只会转瞬即逝，几乎不会有机会进入长期记忆。如果你需要仔细辨认猫的种类，或者确认那个“猫”样的物体究竟是不是猫时，则可能需要系统2的参与。如果你对猫的特征有过深入研究，并且熟悉它们，那么同样猫的品种就已经在系统1的自动处理流水线中处理完成提交到你的脑子里了。而如果你刚刚开始学习辨认猫的品种，捧着一本手册努力查阅或者听一旁的朋友涛涛不觉地解释，就很有可能感受到复杂，并且经历被信息淹没的感觉。对于你而言，那些非常细节的概念之间毫无联系，光是努力维持那些概念在脑子里就十分费力了，然而实际上那些概念刚从系统1的视觉或者听觉提交上来没多久，就直接被丢弃了。 你得承认，复杂超过一定程度，人就几乎不能理解了。 方法我们利用了几种模型来帮助我们分析如何处理复杂性这个问题，这些并不是唯一的解释方法。不过这些模型对于我们的提出解决方法已经有足够的启发了。 根据我们的模型，我们该如何提高处理复杂的能力呢？缩减工作记忆中的概念数量以减少甚至避免和“外界存储”的交换，提高工作记忆的容量以容纳更多的概念数量。 那么如何缩减概念数量呢？我们可以对概念进行分类、抽象、归纳、封装、近似、封装。 分类物以类聚，我们可以把相似的概念分为一类——具有共同特征，或者在时空上接近。这样，我们只需按类别考虑概念。许多苹果、梨、香蕉、西瓜、西红柿、黄瓜、冬瓜、南瓜混在一起。如果我们分别考虑每一个物体，由于物体数量很多，就很难同时考虑。如果我们先按品种先分类，就只需要考虑8堆果蔬，一共8个概念。进一步，我们再按能不能带皮吃分类，则只需要考虑2种概念：能带皮吃和不能带皮的。在此基础上讨论理解，则比同时考虑多个具体概念容易得多。当然分类的方法不止一种，比如按颜色分类、按大小分类，分类的具体原则要根据具体问题决定。适应问题的合理分类，能帮助我们更好的简化模型，梳理思路。 抽象抽象是指把多种概念的共同特征提取出来，从具体的事物体中抽象出概念。比如考虑上面提到的果蔬混合物，在分类的基础上，我们可以提取出水果和蔬菜的概念，这样就能在一般性地研究水果或者蔬菜的特性。有时候，抽象所提取的共同特征太少，导致可以被归类的概念太多、太宽泛，这种抽象可能不一定适合我们想要考虑的问题。我们可以多层抽象，比如将共同特征的要求放严一些，这样抽象得到的概念类别也就更多并且具体些。合理的抽象层次，对模型的组织方式有很大影响。 归纳苹果是甜的，梨也是甜的，西瓜是水果，那么西瓜是甜的吗？我们可以从苹果和梨的特性上尝试归纳出水果是甜的这个特征，以此为假设前提，并通过演绎推广到同样是水果的西瓜上，可以得出西瓜也是甜的的结论。这就是归纳-演绎的常见用法。不过，要小心掉进逻辑的陷阱！归纳-演绎的结构推导出的结论正确，必须建立在归纳的结论正确之上，归纳的结论正确又必须建立在引用事实的正确之上。有的人把假设当事实，自然会归纳得出荒谬的结论；有的人不看具体场景，只觉得要解决的问题与归纳的模式有几分相似便套用模板，自然错到离谱，在考试上俗称套公式。 封装封装是将复杂而紧密联系的概念当成一个整体，也就是当成所谓的黑盒，在考虑更高层次的问题时，仅考虑这个整体与外界的互动，而不必关心它的内部细节。黑盒的概念应用很广。在研究大脑和行为的关系时，就有人通过人接受的刺激和做出的行为来为黑盒建立模型，这种模型是概念上的。在生活上，我们经常也不自觉结合“常识”的给身边的“黑盒”建立模型，比如在炎炎夏日我们进入室内打开空调时，可能会把空调调到最低温度，而不是最适宜温度，这里就隐含了一个我们给空调这个黑盒建立的模型——设定的温度与当前温度温差越大，空调工作的功率越高，然而这不是空调的真实模型。因此，封装并使用简化的黑盒可以缩减概念数量，但是要注意简化后的模型能在多大程度上还原之前的解释，是否符合需要。 分治有时候，许多概念一拥而上，着实令人头疼。但在开始分析之前，先判断是不是这么多概念都是需要同时思考的，是不是可以分而治之。分而治之的思想自古有之，如果我们面对大问题难以入手，那么就化整为零，再逐个击破。同样，面对繁多的概念，如果他们可以分解成更小的概念组，再分别考虑，那么一次要处理的概念数量就能减少不少。 近似近似用哲学的话来说就是：抓住主要矛盾。近似在工程上经常使用。比如我们通过公式推导出了一串复杂的解，我们通常会在误差允许的范围内对解做近似，忽略权重低的项，保留权重高的项。在简化了解的同时在很大程度上保证结论的有效性，并且这让我们能对更有影响力的变量给予更多的注意力。放在实际应用中，我们就能集中资源攻克主要问题。同样，需要注意的是，不要忘记模型的公式本身是对现实的近似，答案本身是对精确解的解释，而如果使用计算机求解，同样要面对有效数字的近似问题。现实在层层近似之后通过仿真展示在我们面前，然而仿真不是真，也不是现实。化简概念也是类似，把细枝末节剔除出去后，剩下重要的概念也应该不会太多。 关于如何扩容，工作记忆这个模型对应着物理的神经系统，想要扩容十分困难。有许多号称可以提高记忆力的方法，其实是也通过减少概念数量。比如，通过谐音的方法将需要记忆的概念连成一个故事或者组成一幅画面，概念之间不再毫无关联，而是形成了一个可以自动播放的整体，回忆时再通过机械记忆住的映射规则（回忆一下小时候背乘法口诀表或者中学背元素周期表）反向解释即可。 应用以编程为例，开始学习时，我们更注意程序语言的语法，通过一些简单的例子体会语言的特性。这时语法上的每一个特征对我们来说都是一个独立的概念，我们需要不断回顾语法规则来加载概念到工作记忆。此时十分费力，也就是入门的阶段。我们在从外界反复使用这些新概念的同时，我们的大脑会根据“重复=重要”的原则把这些概念放入长期记忆中，并且通过反复练习提高熟练度可以提高概念从长期记忆中提取的速度。 在反复练习之中，我们会发现不必过于关注语法本身，而可以总结出基本的使用模式。我们可以抽象出控制结构、循环结构这样的基本结构，明白了可以使用函数来把一系列紧密相关的实现封装起来。我们在阅读别人的代码时，可以不必关注实现细节，而是通过主要函数的调用关系理解整个模块的功能。在这种自底向上的抽象过程中，可能会引入新的编程模型，新的功能模块，新的独立系统。 得益于此，我们可以在高抽象层次上设计系统。我们有能力把握有限的子系统数量，如果子系统的数量过多，可以进一步抽象，保持整体的概念数量可控。","categories":[{"name":"学习","slug":"学习","permalink":"https://blog.kleon.space/categories/学习/"}],"tags":[{"name":"学习","slug":"学习","permalink":"https://blog.kleon.space/tags/学习/"}]},{"title":"如何快速入门新领域","slug":"think/something_new","date":"2020-03-02T22:08:35.000Z","updated":"2020-03-02T22:08:35.000Z","comments":true,"path":"think/something_new/","link":"","permalink":"https://blog.kleon.space/think/something_new/","excerpt":"从零开始学前端（移动端）的过程记录，以及走神想到的关于学习新领域的思考。","text":"从零开始学前端（移动端）的过程记录，以及走神想到的关于学习新领域的思考。 缘起互联网上的垃圾越来越多，各个大厂的生态也趋于闭合。即使仍然还有人在免费提供优质的内容，Google的爬虫也无能为力。现在大行其道的推荐算法，它或许可以通过我的注意力停留时长判断出我的兴趣所在，但在一人一票的制度下，很难为我们挖掘真正沉淀了价值的东西。信息就像一次性纸巾，在被人擦过鼻涕后便只能扔掉，而在一些激励的鼓励下，有不少人在全天无休的制造着这些白色垃圾。 这是我最初开始设想阡陌时的一个初衷：发掘高质量、长生命期的内容。 具体的设想暂且不展开，打造一片理想的数字世界需要技术的支持，而作为数字世界大门的就是前端。 我之前对于前端的了解很少，大概觉得网页就是前端。不过，在各行各业，“前端”都有所知，我之前做硬件开发时的前端便是Verilog HDL。虽然对前端不太了解，但是我觉得计算机软件开发技术可能是所有应用技术里最适合自学入门的，不仅网上现成免费的书籍、课程多，而且得益于开源的兴起，实战项目也不少。 不过这些资源散落在互联网的汪洋大海中，并且其中有一大部分处于公海之外的私人水域中。除此之外，资源的质量如何，难度是否合适，应该何时使用，却少有说明。为数不多的路线地图（Roadmap）也只是笼统而论，还没入门的小白用户看起来自然也就一头雾水，等到七拼八凑，走过了不少弯路，才懵懵懂懂有了些印象。这些是我个人在学习过程中的体会，姑且先推广开来，假设和我有相似遭遇的大有人在吧。 据此至少可以提出三个需求： 第一，我希望有一份详细的路线地图，可以让我对需要学习的领域形成大致的“方向感”； 第二，我希望路线地图拆分成里程碑（Milestone），把一个大目标拆分成小目标，逐个击破，在及时反馈之中增强信心； 第三，我希望学习之路上，有人通行，亦师亦友，汇总常见问题，甚至可以做成专家系统，节约入门者和指导者的时间。 评估首先，分析我的位置： 掌握Python、C++等编程语言。 了解计算机网络基本常识。 熟悉搜索引擎的使用方法🤣。 其次，权衡我的目标： 网页端、移动端还是桌面端？ 这其实是在考虑这三种平台优势是什么。 网页端适合电脑使用，对实时性要求不高，合适生产力模式下使用。 移动端便携性好，普及程度广，合适随身模式使用，但可能被娱乐模式干扰，优先级较高。 桌面端适合电脑使用，有更强的性能与定制性。 现有主流框架有哪些？是否有跨平台框架？ 网页端有React、Angular、Vue，移动端有Android/iOS原生，桌面端各个系统各式各样。跨平台的方案有React Native、Electron、Flutter。 各个框架的优势是什么？ 对比主要从代码复用性和社区成熟度上考虑。代码复用性包括跨平台代码复用，可用第三方库。社区成熟度包括文档、教程的完善程度，项目活跃度，是否仍在积极维护。框架的选择也是见仁见智，JS框架在性能上可能不如意，移动原生开发需要的学习量和工作量太过巨大，跨平台方案不太成熟。评估之后还是选择移动端切入，日后可以补充JS框架做网页端。 这里的问题是，我从网上获得的评论都具有主观性，对前端几乎为0的了解也导致我没办法做出合理判断。再者，技术只是实现目标的手段。总之，先淌一淌这浑水再做决定。 路线学习因目标不同，知识类型不同，有许多不同的方法，从大的方向上可以分为三个阶段。 直接上手： 这适合早期入门。在老师的指导下或参考有效的资料，形成对知识的感性认识。常见的学习模式是：你不要问那么多为什么，照做就是。很多时候，通过这种方法，就能大致看懂别人的成果并简单应用了。并且，通过上手-反馈循环，可以积累信心。虽然大多数知识的入门都可以使用这个方法，但通过这个方法学到的知识不系统，存在逻辑缺失，在学习的时候可能会在心里存下不少疑问。比如在出现复杂问题时，可能无法解释，只能诉诸玄学。这就是缺乏系统化、整体化的认知的结果。 系统学习： 这适合中期进阶。之所以能系统学习，是因为知识体系相对稳定，有大量前人总结的体系模型可以参考。系统化学习通常来说要明确学习领域和起点，预先给出公理或者假设，在大家都明确了以下所有内容均基于以上公理或者假设——也就是明确了知识体系的适用范围之后，开始引出推论和模型，并且通过严格的逻辑证明推论成立，或者通过模型解释实际案例表示模型有效。经过系统学习，基本上能对大部分问题套用模型分析解释，解决问题，达成目标，有了更强的定制能力。在系统学习的过程中，可以解释之前遇到的不少疑惑，但系统学习需要警惕绝对知识论，需要明确知识是在一定假设上的理想模型，并不是绝对真理。如果批判性地思考学习，心中的疑问可能只增不减，但这并不是一件完全的坏事，这说明对知识有了更全面深刻的认识。所谓知道的越多，才发现不知道得更多。在系统学习过程中，有人不能接受完全接受模型，而是会对为什么提出模型产生疑问，因为知识体系是经过无数人试错后的总结，而了解知识诞生的历史会让我们学习知识的逻辑更加顺畅，因为建立知识体系的历史是探索与偶然的历史。系统学习并不是学习的终点，已知的模型在面对新的问题的时候很可能会失效，这时候就需要科学探究。 科学探究： 这适合后期探索。之所以带上科学二字，是因为需要使用科学研究方法，并且得出的知识模型可证伪。不可证伪的知识通常是万能解释的，比如很多伪心理学总能用一套看似有道理的解释套用到所有情况上，并且没法提出切实可行、可重复的解决方案。使用这种方法需要具备丰富的想象力和严谨的逻辑。丰富的想象力要求有能力补足未知的细节，建立知识模型，并设计证明方法，对结果有预期。严谨的逻辑需要知识模型具有可解释性，以前人已有的知识或者可理解的假设作为起点，构建一条完整的逻辑链路。科学探究能构建理论解释之前的一部分疑问，但却有可能调入疑问的黑洞之中，不能说科学的尽头是神学，但是面对自然，仍然要抱有敬畏之心。虽然我们有了一些用来抵御未知的知识，但也要意识到我们这片知识空间的狭小和脆弱，在它之外的是漫无边际、比黑更黑的未知。 希望以上这些可能对处于不同学习阶段的人有点启发，当然这并不是说所有的学习过程都要以第科学探究为最终目标，需要根据自己的实际的目标和所处的位置选择合适的方法。比如纯粹建立在逻辑上的知识可能就不能直接上手，只能系统学习了。又比如应用性强的知识适合直接上手，即所谓自学成才，可能身经百战，但不知其然，但也可以系统学习，即所谓科班出身，可能头头是道，但纸上谈兵。 值得警惕的是那些打着科学旗号兜售自己的“万灵药”的人。 回到前端入门上来，因为是应用性很强的知识，所以我选择直接上手，需要深度定制或者遇到难解的问题，再回头补习。 上手搜了些路线图，有些前人的路可以参考，但完全照搬可能有些拘束，根据自身情况借鉴即可。 第一步：环境准备。这是几乎学习新编程语言必备的，不同教程大同小异，如果社区成熟，按照文档做一遍就行。一些特殊的网络问题，需要想办法解决，同样成熟的社区也会有相应的解决方案。 第二步：玩具教程，这通常也是出现在社区文档中的。玩具，又称小玩意儿，通过这些简单的小玩意儿，能大概有个端到端的成果可以展示了。可不要到此就说自己精通了哦。关于语法什么的，如果有相关的编程基础（概念都是相通的，语法存在差异），大概也能猜个大差不差，如果只知道面向对象，而学习函数式可能理解起来就有点费劲，不妨从基础概念开始理解。 第三步：了解常用组件，通过但不限于高质量的知识沉淀网站，风靡全球的代码托管网站获得基本了解，如果能找到好的起步教程就很好。一个理想的教程符合直接上手的原则，在上手过程中，介绍了常用的组件、使用方式，也就是“套路”。如果这一步有概念缺失，需要自行回溯学习前置知识，只需要回溯到连蒙带猜理解个大差不差的程度就可以回来继续教程了。 第四步：好了，你已经知道了1+1=2，下面我们来算个不定积分。这是不少人可能遇到的情况，教程也能看了七七八八，却发现离自己想实现的目标还有不小的差距。这个差距通常是几方面方面引起的：不知道某项功能怎么实现，需要定制第三方库，需要自己完全实现。 我们需要逐个击破： 不知道用什么轮子：搜索学习更复杂的教程，最好包含自己想实现的功能，或者其中一部分。 有轮子但需要调整：回溯学习第三方库相关知识，尝试修改并观察结果，理解基本原理直到可以上手修改。 找不到轮子：回溯学习所需的知识，自底向上，逐步搭建。 其实到这一步，也算淌了些水，我们可以先停一停，需要回到评估这一步。如果轮子缺失严重——这通常意味着社区还不够成熟以及巨大的工作量，可能需要考虑使用别的方案，这很有可能意味着评估阶段的工作不足，或者领域太新。如果重新评估后发现其他方案更完善，那么应该切换方案。可能有人会抱怨以上四步白白浪费了时间，其实可以从两方面来考虑。其一，这四步消耗的时间是沉没成本，没办法收回，最合理的做法是忽视它，评估继续这个方案所需要的投入以及其他方案从0开始的投入哪个更多。其二，面对一个问题的多种解决方案，通常在回溯之后的知识上是相通的、可重用的，时间也不是完全浪费的。 我学到这里，基本上达到了可以使用轮子搭建自己想要的功能的地步。向后就需要根据设计的功能反复使用第四步的三个方法，直到满足我们的设计，或者修改设计降低实现复杂度。 后续我在这篇文章里，没有重点讲具体的知识点，而是想通过这次的学习实践归纳一般的方法。当然，抽象得足够高的规律都可能变成“万灵药”的危险。 前端先学了一点皮毛，先暂且用着，还要补些网页前端、后端、设计、认知科学方面的知识。","categories":[{"name":"学习","slug":"学习","permalink":"https://blog.kleon.space/categories/学习/"}],"tags":[{"name":"学习","slug":"学习","permalink":"https://blog.kleon.space/tags/学习/"}]}]}